{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "from typing import List, Optional, Tuple, Union\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "from utils import get_classifier, generate_next, concat_past, expand_past\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    l = list()\n",
    "    for line in open(filename):\n",
    "        l.append(line.strip())\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main():  # the following n cells\n",
    "\n",
    "seed = 0\n",
    "device = \"cuda\"\n",
    "\n",
    "pretrained_model = \"gpt2-medium\"\n",
    "# sentiment: class_label = [\"positive\", \"negative\", \"very position\", \"very negative\", \"neutral\"] \n",
    "discrim = \"sentiment\"\n",
    "class_label = 3\n",
    "num_of_triggers = 1\n",
    "\n",
    "trigger_format = \"key_value\"  # token or key_value\n",
    "TRIGGER_POSITION_ID = 0  # for position reset\n",
    "\n",
    "num_epochs = 1\n",
    "num_iterations = 2\n",
    "learning_rate = 5e-3\n",
    "# learning_rate = 0  # baseline (no real trigger)\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "sample = True\n",
    "gumbel_softmax = True\n",
    "detach = True\n",
    "# ----------------------\n",
    "reset_pos_emb = True\n",
    "not_mask_trigger = False\n",
    "gumbel_temperature = 1.0\n",
    "\n",
    "batch_size = 4\n",
    "multiple_input = True\n",
    "\n",
    "# more or less fixed for generation\n",
    "top_k = 10\n",
    "temperature = 1.0\n",
    "repetition_penalty = 1.0\n",
    "adam_epsilon = 1e-8\n",
    "max_grad_norm = 1.0\n",
    "length = 40\n",
    "\n",
    "if detach and not gumbel_softmax:\n",
    "    assert False, \"require gumbel softmax when using detach\"\n",
    "\n",
    "verbose = True\n",
    "check_real_loss = True\n",
    "\n",
    "# data path\n",
    "train_filename = \"persona_train.txt\"\n",
    "eval_filename = \"persona_eval.txt\"\n",
    "    \n",
    "# WARNING: GPT2 only\n",
    "new_line_idx = 198  # '\\n'\n",
    "new_line_idx_1 = 628  # '\\n\\n'\n",
    "stop_token = \".\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# load pretrained model\n",
    "model = GPT2LMHeadModel.from_pretrained(pretrained_model, output_hidden_states=True)\n",
    "model.to(device)\n",
    "model.eval()  # do not need batchnorm or dropout layers for training/eval\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model)\n",
    "\n",
    "# Freeze GPT-2 weights\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "classifier, class_id = get_classifier(discrim, class_label, device)\n",
    "\n",
    "num_layers = model.config.n_layer\n",
    "\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "lm_bos_output = model(torch.tensor(tokenizer.encode(tokenizer.bos_token), dtype=torch.long, device=device).unsqueeze(0).repeat(batch_size, 1))  # BOS\n",
    "\n",
    "# WARNING: GPT2 only\n",
    "t_pad_token = tokenizer.bos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cond_text_list = [\"This is a terrible restaurant.\", \"I like drinking water.\", \"It is raining today.\", \"I'm doing my homework.\"]\n",
    "# cond_text_list = [\"I don't like this restaurant.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize trigger\n",
    "# Note: since we use the same trigger for all inputs in a batch, we only create/register trigger(s) for one and repeat it\n",
    "def init_trigger(model, tokenizer, num_of_triggers, trigger_format):\n",
    "    if num_of_triggers > 0:\n",
    "        if trigger_format == \"token\":  # learn a continuous embedding\n",
    "            trigger_embedding_list = []\n",
    "            for _ in range(num_of_triggers):\n",
    "                trigger_embedding_i = copy.deepcopy(model.transformer.wte(\n",
    "                    torch.tensor(tokenizer.encode(tokenizer.bos_token), device=device, dtype=torch.long).unsqueeze(0)))\n",
    "                trigger_embedding_list.append(trigger_embedding_i)\n",
    "            ori_trigger_embedding = nn.Parameter(torch.cat(trigger_embedding_list, dim=1))  # bze x n x emb_size\n",
    "            model.ori_trigger_embedding = ori_trigger_embedding  # register to the model (optimizer)\n",
    "    #         trigger_embedding = trigger_embedding.repeat(batch_size, 1, 1)  # cannot do it here, otherwise trigger_embedding becomes a non-leaf node where the grad will not backprop\n",
    "        elif trigger_format == \"key_value\":  # learn key values\n",
    "            ori_trigger_key_values = [(None, None) for _ in range(num_layers)]\n",
    "            bos_key_values = model(torch.tensor(tokenizer.encode(tokenizer.bos_token), dtype=torch.long).unsqueeze(0).to(device))[\n",
    "                            \"past_key_values\"]\n",
    "            for layer in range(num_layers):\n",
    "                for i_t in range(num_of_triggers):\n",
    "                    trigger_i_key_value = copy.deepcopy(bos_key_values)\n",
    "                    # key, value shape: bze, num_heads, seq_len, embed_per_head\n",
    "                    trigger_i_key, trigger_i_value = nn.Parameter(trigger_i_key_value[layer][0]), \\\n",
    "                                                     nn.Parameter(trigger_i_key_value[layer][1])\n",
    "\n",
    "                    trigger_i_key.requires_grad = True\n",
    "                    trigger_i_value.requires_grad = True\n",
    "\n",
    "                    if ori_trigger_key_values[layer][0] is None:\n",
    "                        ori_trigger_key_values[layer] = (trigger_i_key, trigger_i_value)\n",
    "                    else:\n",
    "                        # if multiple triggers\n",
    "                        trigger_key = nn.Parameter(torch.cat((ori_trigger_key_values[layer][0], trigger_i_key), dim=-2))\n",
    "                        trigger_value = nn.Parameter(torch.cat((ori_trigger_key_values[layer][1], trigger_i_value), dim=-2))\n",
    "                        ori_trigger_key_values[layer] = (trigger_key, trigger_value)\n",
    "\n",
    "                # register parameter into optimizer\n",
    "                key_name = \"l_%d_key\" % layer\n",
    "                value_name = \"l_%d_value\" % layer\n",
    "                if num_of_triggers == 1:\n",
    "                    model.register_parameter(name=key_name, param=trigger_i_key)\n",
    "                    model.register_parameter(name=value_name, param=trigger_i_value)\n",
    "                else:\n",
    "                    model.register_parameter(name=key_name, param=trigger_key)\n",
    "                    model.register_parameter(name=value_name, param=trigger_value)\n",
    "\n",
    "            ori_trigger_key_values = tuple(ori_trigger_key_values)\n",
    "            model.ori_trigger_key_values = ori_trigger_key_values\n",
    "    #         trigger_key_values = expand_past(trigger_key_values, num_layers, batch_size)  # similar to trigger_embedding, need leaf level grad\n",
    "        else:\n",
    "            assert False, \"trigger_format: %s not supported\" % trigger_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimizing params: \n",
      "l_0_key l_0_value l_1_key l_1_value l_2_key l_2_value l_3_key l_3_value l_4_key l_4_value l_5_key l_5_value l_6_key l_6_value l_7_key l_7_value l_8_key l_8_value l_9_key l_9_value l_10_key l_10_value l_11_key l_11_value l_12_key l_12_value l_13_key l_13_value l_14_key l_14_value l_15_key l_15_value l_16_key l_16_value l_17_key l_17_value l_18_key l_18_value l_19_key l_19_value l_20_key l_20_value l_21_key l_21_value l_22_key l_22_value l_23_key l_23_value\n"
     ]
    }
   ],
   "source": [
    "init_trigger(model, tokenizer, num_of_triggers, trigger_format)\n",
    "\n",
    "# optimizer\n",
    "param_optimizer = list(filter(lambda p: p[1].requires_grad, list(model.named_parameters())))\n",
    "\n",
    "# debugging: get all optimized param names\n",
    "print(\"optimizing params: \")\n",
    "print(\" \".join(o[0] for o in param_optimizer))\n",
    "\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "        'weight_decay': 0.0,\n",
    "    },\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                  lr=learning_rate,\n",
    "                  eps=adam_epsilon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_inputs(batch_cond_text_list, tokenizer, device, t_pad_token):\n",
    "    batch_max_length = 0\n",
    "    batch_min_length = 10000\n",
    "    batch_input_ids = list()\n",
    "    all_inputs, all_attention_masks, all_lengths, all_padding_length = list(), list(), list(), list()\n",
    "    padding_token = tokenizer.encode(t_pad_token)[0]  # WARNING: BOS is for GPT2 only. Should use padding token\n",
    "    for cond_text in batch_cond_text_list:\n",
    "        inputs_ids = tokenizer.encode(tokenizer.bos_token + cond_text)\n",
    "        batch_max_length = len(inputs_ids) if len(inputs_ids) > batch_max_length else batch_max_length\n",
    "        batch_min_length = len(inputs_ids) if len(inputs_ids) < batch_min_length else batch_min_length\n",
    "        batch_input_ids.append(inputs_ids)\n",
    "    for inputs_ids in batch_input_ids:\n",
    "        all_lengths.append(len(inputs_ids))\n",
    "        padding_len = batch_max_length - len(inputs_ids)\n",
    "        attention_mask = [1] * len(inputs_ids) + [0] * padding_len\n",
    "        inputs_ids = inputs_ids + [padding_token] * padding_len \n",
    "\n",
    "        all_padding_length.append(padding_len)\n",
    "        all_inputs.append(inputs_ids)\n",
    "        all_attention_masks.append(attention_mask)\n",
    "\n",
    "    all_input_ids = torch.tensor(all_inputs, dtype=torch.long, device=device)\n",
    "    all_attention_masks = torch.tensor(all_attention_masks, dtype=torch.long, device=device)\n",
    "    \n",
    "    return all_input_ids, all_attention_masks, batch_min_length, batch_max_length, all_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def penalize_new_line(logits):\n",
    "    new_line_tokens = [new_line_idx, new_line_idx_1]\n",
    "    for b_i in range(logits.shape[0]):\n",
    "        for nt in new_line_tokens:\n",
    "            if logits[b_i, -1, nt] < 0:\n",
    "                logits[b_i, -1, nt] *= 5\n",
    "            else:\n",
    "                logits[b_i, -1, nt] /= 5\n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_prompt_response(model, tokenizer, mode, lm_bos_output, batch_size, device, class_label,\n",
    "                             num_iterations, learning_rate, gradient_accumulation_steps, sample, gumbel_softmax,\n",
    "                             detach, reset_pos_emb, not_mask_trigger, gumbel_temperature, top_k, temperature, \n",
    "                             repetition_penalty, adam_epsilon, max_grad_norm, length, t_pad_token, stop_token,\n",
    "                             num_epochs, context_list, verbose, check_real_loss,\n",
    "                             seed,\n",
    "                            ):\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    if mode == \"eval\":\n",
    "        num_epochs = 1\n",
    "        \n",
    "    total_data_batches = len(context_list) // batch_size\n",
    "        \n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"&&&&& epoch: %d &&&&&\" % (epoch + 1))\n",
    "        \n",
    "#         if mode == \"train\":\n",
    "#             random.shuffle(context_list)\n",
    "            \n",
    "        epoch_loss = 0\n",
    "            \n",
    "        for cond_list_idx in range(total_data_batches):\n",
    "            cond_list = context_list[cond_list_idx * batch_size: (cond_list_idx+1) * batch_size]\n",
    "            all_input_ids, all_attention_masks, batch_min_length, batch_max_length, all_lengths = prep_inputs(cond_list, tokenizer, device, t_pad_token)\n",
    "            \n",
    "            model.zero_grad()\n",
    "\n",
    "            loss_per_update = 0\n",
    "            total_loss = 0\n",
    "\n",
    "            if reset_pos_emb:\n",
    "                c_position_ids = torch.arange(1, batch_min_length - 1, dtype=torch.long, device=device)\n",
    "                c_position_ids = c_position_ids.unsqueeze(0).repeat(batch_size, 1)\n",
    "            else:\n",
    "                c_position_ids = None\n",
    "\n",
    "            if reset_pos_emb:\n",
    "                t_position_ids = torch.ones(batch_size, num_of_triggers).to(torch.long).to(device) * TRIGGER_POSITION_ID\n",
    "            else:\n",
    "                t_position_ids = None\n",
    "\n",
    "            all_context_prompts = []\n",
    "            all_generated_prompt_length = []\n",
    "\n",
    "            for i in range(num_iterations):\n",
    "\n",
    "                past = lm_bos_output[\"past_key_values\"]\n",
    "\n",
    "                if num_of_triggers > 0:\n",
    "                    if trigger_format == \"token\":\n",
    "                        trigger_embedding = model.ori_trigger_embedding.repeat(batch_size, 1, 1)\n",
    "                        lm_trigger_output = model(inputs_embeds=trigger_embedding, position_ids=t_position_ids)\n",
    "                        trigger_key_values = lm_trigger_output[\"past_key_values\"]\n",
    "                    else:\n",
    "                        trigger_key_values = expand_past(model.ori_trigger_key_values, num_layers, batch_size)\n",
    "                    past = concat_past(past, trigger_key_values, num_layers)\n",
    "                else:\n",
    "                    trigger_key_values = None\n",
    "\n",
    "                output_so_far = all_input_ids[:, :batch_min_length]  # bze x (batch_min_length - 1)\n",
    "\n",
    "                context_lm_output = model(all_input_ids[:, 1: batch_min_length - 1], past_key_values=past, position_ids=c_position_ids, )\n",
    "\n",
    "                past = context_lm_output[\"past_key_values\"]\n",
    "                last = output_so_far[:, batch_min_length - 1: batch_min_length]\n",
    "\n",
    "                gumbel_vector = None\n",
    "                if detach:\n",
    "                    all_gumbel_vectors = None\n",
    "\n",
    "                prompt_not_done = torch.ones(batch_size, 1, dtype=torch.uint8, device=device)\n",
    "                generated_prompt_length = torch.zeros(batch_size, 1, dtype=torch.long, device=device)\n",
    "                prompt_stop_first = True\n",
    "\n",
    "                # generate conditional prompt\n",
    "                if verbose:\n",
    "                    print(\"=====Epoch: %d; data_batch: %d; Iteration: %d=====\" % (epoch + 1, cond_list_idx + 1, i + 1))\n",
    "\n",
    "                for p_i in range(length):\n",
    "                    if reset_pos_emb:\n",
    "                        past_length = past[0][0].size(-2)\n",
    "                        p_position_ids = torch.arange(past_length - num_of_triggers, past_length - num_of_triggers + 1, dtype=torch.long, device=device)\n",
    "                        p_position_ids = p_position_ids.unsqueeze(0).repeat(batch_size, 1)\n",
    "                    else:\n",
    "                        p_position_ids = None\n",
    "                    if gumbel_softmax and gumbel_vector is not None:\n",
    "                        last_emb = torch.mm(gumbel_vector, model.transformer.wte.weight).unsqueeze(1)  # needs to be bze, n, emb\n",
    "                        lm_output = model(inputs_embeds=last_emb, past_key_values=past, position_ids=p_position_ids)\n",
    "                    else:\n",
    "                        lm_output = model(last, past_key_values=past, position_ids=p_position_ids)\n",
    "\n",
    "                    logits, past, all_hidden = (\n",
    "                        lm_output[\"logits\"],  # bze, cur_seq_len, vocab_size\n",
    "                        lm_output[\"past_key_values\"],  # acc_seq_len\n",
    "                        lm_output[\"hidden_states\"],  # num_layers + 1, tuple of (bze, cur_seq_len, hid_sze)\n",
    "                    )\n",
    "\n",
    "                    vocab_size = logits.shape[-1]\n",
    "\n",
    "                    logits = penalize_new_line(logits)\n",
    "\n",
    "                    # last: bze x 1; gumbel_vector: bze x vocab_size\n",
    "                    last, gumbel_vector = generate_next(logits, output_so_far, top_k=top_k, temperature=temperature, \n",
    "                                                        repetition_penalty=repetition_penalty, sample=sample, \n",
    "                                                        gumbel_softmax=gumbel_softmax, gumbel_temperature=gumbel_temperature, detach=detach)\n",
    "                    # manually assign end token is too long\n",
    "                    if p_i == length - 1:\n",
    "                        for m_b_i in range(batch_size):\n",
    "                            if generated_prompt_length[m_b_i] == 0:\n",
    "                                last[m_b_i] = tokenizer.encode(stop_token)[0]  # encode outputs a list (1 element)\n",
    "                                if gumbel_softmax:\n",
    "                                    gumbel_vector[m_b_i] = F.one_hot(torch.tensor(tokenizer.encode(stop_token), dtype=torch.long, device=device), num_classes=vocab_size)\n",
    "\n",
    "                    # double check the length (p_i vs. lengths below)\n",
    "                    is_generated = torch.tensor(all_lengths, device=device).unsqueeze(-1) <= (p_i + batch_min_length)  # bze x 1. is generated or stil in the context\n",
    "                    is_end_token = last == torch.tensor(tokenizer.encode(stop_token), device=device)  # bze x 1\n",
    "                    is_actually_ending = is_generated * is_end_token\n",
    "\n",
    "                    # keep track of prompt length\n",
    "                    generated_prompt_length = generated_prompt_length + prompt_not_done * is_actually_ending * p_i\n",
    "                    \n",
    "                    # if generated, use the generated token as last; otherwise (from the original), copy the orignal token/gumbel_vector\n",
    "                    if batch_min_length + p_i < all_input_ids.shape[1]:\n",
    "                        last = last * is_generated + all_input_ids[:, batch_min_length + p_i].unsqueeze(1) * (~is_generated)  # is_generated is bool. need to use \"~\" instead of (1-is_generated)\n",
    "                    else:\n",
    "                        last = last\n",
    "\n",
    "                    if gumbel_softmax:\n",
    "                        if batch_min_length + p_i < all_input_ids.shape[1]:\n",
    "                            ori_one_hot = F.one_hot(all_input_ids[:, batch_min_length + p_i], num_classes=vocab_size)  # bze x vocab_size \n",
    "                            gumbel_vector = gumbel_vector * is_generated + ori_one_hot * (~is_generated)\n",
    "                        else:\n",
    "                            gumbel_vector = gumbel_vector\n",
    "                    if prompt_stop_first and torch.sum(is_actually_ending) > 0:\n",
    "                        prompt_stop_first = False\n",
    "                        min_p_past = past\n",
    "                        min_p_last = last\n",
    "                        min_gumbel_vector = gumbel_vector\n",
    "\n",
    "                    if detach: \n",
    "                        # WARNING: This can be quite large\n",
    "                        if all_gumbel_vectors is None:\n",
    "                            all_gumbel_vectors = gumbel_vector.unsqueeze(1)  # bze x 1 x vocab_size\n",
    "                        else:\n",
    "                            all_gumbel_vectors = torch.cat((all_gumbel_vectors, gumbel_vector.unsqueeze(1)), dim=1)  # bze x n x vocab_size\n",
    "\n",
    "                    output_so_far = torch.cat((output_so_far, last), dim=1)  # bze x length\n",
    "\n",
    "                    prompt_not_done = prompt_not_done * (~is_actually_ending)  # to check is we need to stop by summing\n",
    "\n",
    "                    if torch.sum(prompt_not_done) == 0:\n",
    "                        break\n",
    "\n",
    "                if verbose:\n",
    "                    print(\"context + prompt\")\n",
    "                    batch_context_prompts = []\n",
    "                    for cp_i in range(batch_size):\n",
    "                        cp_i_text = tokenizer.decode(output_so_far[cp_i][:(generated_prompt_length[cp_i].item() + 1 + batch_min_length)].tolist())\n",
    "                        print(cp_i_text)\n",
    "                        batch_context_prompts.append(cp_i_text)\n",
    "                    print(\"***\" * 20)\n",
    "\n",
    "                all_context_prompts.append(batch_context_prompts)\n",
    "                all_generated_prompt_length.append(generated_prompt_length + batch_min_length + 1)\n",
    "\n",
    "                if mode == \"eval\":  # only need context and prompts for evaluation\n",
    "                    continue\n",
    "\n",
    "                cp_min_length = torch.min(generated_prompt_length.squeeze(1)) + batch_min_length + 1\n",
    "\n",
    "                if detach:\n",
    "                    detach_context_output = model(all_input_ids[:, :batch_min_length])\n",
    "                    past = detach_context_output[\"past_key_values\"]\n",
    "                    all_gumbel_embeddings = torch.matmul(all_gumbel_vectors[:, :cp_min_length - batch_min_length - 1, :], model.transformer.wte.weight)\n",
    "                    last_detach_output = model(inputs_embeds=all_gumbel_embeddings, past_key_values=past)\n",
    "                    past = last_detach_output[\"past_key_values\"]\n",
    "\n",
    "                    gumbel_vector = min_gumbel_vector\n",
    "                else:\n",
    "                    # adjust past, last, and gumbel_vector\n",
    "                    past = min_p_past\n",
    "                    last = min_p_last\n",
    "\n",
    "\n",
    "                everything_so_far = output_so_far[:, :cp_min_length]\n",
    "\n",
    "                # generate response\n",
    "                response_hidden = None\n",
    "                response_so_far = None\n",
    "                first = True\n",
    "                to_break = False\n",
    "\n",
    "                response_not_done = torch.ones(batch_size, 1, dtype=torch.uint8, device=device)\n",
    "                generated_response_length = torch.zeros(batch_size, 1, dtype=torch.long, device=device)\n",
    "                response_stop_first = True\n",
    "\n",
    "                for r_i in range(length):\n",
    "                    # TODO: mask trigger key and value\n",
    "                    if num_of_triggers > 0 and not not_mask_trigger and not detach:\n",
    "                        # create attention mask\n",
    "                        past_length = past[0][0].shape[-2]\n",
    "                        attention_mask = torch.ones(batch_size, past_length + 1)  # add current 1 to length\n",
    "                        attention_mask[:, 1: 1 + num_of_triggers] = 0  # bze=1, the first element is BOS\n",
    "                        attention_mask = attention_mask.to(device)\n",
    "                    else:\n",
    "                        attention_mask = None\n",
    "\n",
    "                    if num_of_triggers > 0 and reset_pos_emb and not detach:\n",
    "                        past_length = past[0][0].size(-2)\n",
    "                        r_position_ids = torch.arange(past_length - num_of_triggers, past_length - num_of_triggers + 1, dtype=torch.long, device=device)\n",
    "                        r_position_ids = r_position_ids.unsqueeze(0).repeat(batch_size, 1)\n",
    "                    else:\n",
    "                        r_position_ids = None\n",
    "\n",
    "                    # debugging\n",
    "                    if gumbel_softmax:\n",
    "                        last_emb = torch.mm(gumbel_vector, model.transformer.wte.weight).unsqueeze(1)  # bze x 1 x hidden\n",
    "                        lm_rep_output = model(inputs_embeds=last_emb, past_key_values=past, attention_mask=attention_mask,\n",
    "                                          output_attentions=True, position_ids=r_position_ids)\n",
    "                    else:\n",
    "                        lm_rep_output = model(last, past_key_values=past, attention_mask=attention_mask, output_attentions=True,\n",
    "                                          position_ids=r_position_ids)\n",
    "\n",
    "                    rep_logits, past, rep_all_hidden = (\n",
    "                        lm_rep_output[\"logits\"],  # bze, cur_seq_len, vocab_size\n",
    "                        lm_rep_output[\"past_key_values\"],  # acc_seq_len\n",
    "                        lm_rep_output[\"hidden_states\"],  # num_layers + 1, tuple of (bze, cur_seq_len, hid_sze)\n",
    "                    )\n",
    "\n",
    "                    rep_logits = penalize_new_line(rep_logits)\n",
    "\n",
    "                    last, gumbel_vector = generate_next(rep_logits, everything_so_far, top_k=top_k, temperature=temperature, \n",
    "                                                        repetition_penalty=repetition_penalty, sample=sample, \n",
    "                                                        gumbel_softmax=gumbel_softmax, gumbel_temperature=gumbel_temperature, detach=detach)\n",
    "\n",
    "                    last_hidden = rep_all_hidden[-1]\n",
    "\n",
    "                    if response_hidden is None:\n",
    "                        response_hidden = last_hidden\n",
    "                    else:\n",
    "                        response_hidden = torch.cat((response_hidden, last_hidden), dim=1)  # bze, n, hid_size\n",
    "\n",
    "                    if to_break:\n",
    "                        break\n",
    "\n",
    "                    # manually assign end token is too long\n",
    "                    if r_i == length - 1:\n",
    "                        for r_m_b_i in range(batch_size):\n",
    "                            if generated_response_length[r_m_b_i] == 0:\n",
    "                                last[r_m_b_i] = tokenizer.encode(stop_token)[0]  # encode outputs a list (1 element)\n",
    "                                gumbel_vector[r_m_b_i] = F.one_hot(torch.tensor(tokenizer.encode(stop_token), dtype=torch.long, device=device), num_classes=vocab_size)\n",
    "\n",
    "                    # adjust \n",
    "                    r_is_generated = generated_prompt_length + 1 + batch_min_length <= (r_i + cp_min_length)  # bze x 1. is generated or stil in the context+prompt\n",
    "                    r_is_end_token = last == torch.tensor(tokenizer.encode(stop_token), device=device)  # bze x 1\n",
    "                    r_is_actually_ending = r_is_generated * r_is_end_token\n",
    "\n",
    "                    # keep track of response length\n",
    "                    generated_response_length = generated_response_length + response_not_done * r_is_actually_ending * r_i\n",
    "\n",
    "                    # if generated, use the generated token as last; otherwise (from context+prompt), copy the orignal token/gumbel_vector\n",
    "                    if cp_min_length + r_i < output_so_far.shape[1]:\n",
    "                        last = last * r_is_generated + output_so_far[:, cp_min_length + r_i].unsqueeze(1) * (~r_is_generated)\n",
    "                    else:\n",
    "                        last = last\n",
    "\n",
    "                    if gumbel_softmax:\n",
    "                        if cp_min_length - batch_min_length + r_i < all_gumbel_vectors.shape[1]:\n",
    "                            gumbel_vector = gumbel_vector * r_is_generated + all_gumbel_vectors[:, cp_min_length - batch_min_length + r_i, :] * (~r_is_generated)                \n",
    "                        else:\n",
    "                            gumbel_vector = gumbel_vector\n",
    "\n",
    "                    everything_so_far = torch.cat((everything_so_far, last), dim=1)\n",
    "\n",
    "                    response_not_done = response_not_done * (~r_is_actually_ending)  # to check is we need to stop by summing\n",
    "                    if torch.sum(response_not_done) == 0:\n",
    "                        to_break = True\n",
    "                \n",
    "                if verbose:\n",
    "                    print(\"response: \")\n",
    "                    for pr_i in range(batch_size):\n",
    "                        if generated_response_length[pr_i] == 0:\n",
    "                            print(tokenizer.decode(everything_so_far[pr_i][generated_prompt_length[pr_i].item() + 1 + batch_min_length:].tolist()))  # exceeds the max length set (so generated_prompt_length is 0)\n",
    "                        else:\n",
    "                            print(tokenizer.decode(everything_so_far[pr_i][generated_prompt_length[pr_i].item() + 1 + batch_min_length:(generated_response_length[pr_i].item() + 1 + cp_min_length)].tolist()))\n",
    "                    print(\"***\" * 20)\n",
    "\n",
    "                extracted_hidden = None\n",
    "                # hidden: bze, 1, hid_size\n",
    "                for hb_i in range(batch_size):\n",
    "                    hb_i_start = generated_prompt_length[hb_i] + 1 + batch_min_length - cp_min_length + 1\n",
    "                    hb_i_end = generated_response_length[hb_i] + 1 + 1\n",
    "                    hb_i_hidden = torch.mean(response_hidden[hb_i:hb_i+1, hb_i_start:hb_i_end, :], dim=1)  # 1, hid_size \n",
    "                    if extracted_hidden is None:\n",
    "                        extracted_hidden = hb_i_hidden\n",
    "                    else:\n",
    "                        extracted_hidden = torch.cat((extracted_hidden, hb_i_hidden), dim=0)\n",
    "\n",
    "                prediction = classifier(extracted_hidden)\n",
    "                label = torch.tensor([class_label], device=device, dtype=torch.long).repeat(batch_size)\n",
    "                discrim_loss = ce_loss(prediction, label)\n",
    "                \n",
    "                if verbose:\n",
    "                    print(\"discrim loss: %.6f\" % discrim_loss.data.cpu().numpy())\n",
    "                    loss_per_update += discrim_loss.item()\n",
    "\n",
    "                if num_of_triggers > 0:\n",
    "                    # compute gradients\n",
    "                    discrim_loss.backward()\n",
    "\n",
    "                    # # debugging: check grad\n",
    "                    if trigger_format == \"token\":\n",
    "                        print_debug = False\n",
    "                        if print_debug:\n",
    "                            print(\"token grad\")\n",
    "                            print(model.ori_trigger_embedding.grad)\n",
    "                #             print(trigger_embedding)\n",
    "                        #\n",
    "                            # debugging\n",
    "                            print(\"original trigger embedding\")\n",
    "                            print(model.ori_trigger_embedding)\n",
    "                    # else:\n",
    "                    #     print(\"trigger_key_value_grad\")\n",
    "                    #     # print(model.l_12_key_0.grad.shape)\n",
    "                    #     # print(model.l_12_key_0.grad)\n",
    "                    #     print(model.l_12_value.grad.shape)\n",
    "                    #     # print(model.l_12_value_1.grad)\n",
    "                    #     print(model.l_12_value)\n",
    "\n",
    "                    if (i + 1) % gradient_accumulation_steps == 0:\n",
    "                        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "\n",
    "                        optimizer.step()\n",
    "                        model.zero_grad()\n",
    "                        if verbose:\n",
    "                            print(\"\\n=======update loss: %.6f=======\" % (loss_per_update / gradient_accumulation_steps))\n",
    "                        total_loss += loss_per_update\n",
    "                        loss_per_update = 0\n",
    "\n",
    "                    # # debugging\n",
    "                    # print(\"new trigger embedding\")\n",
    "                    # print(trigger_embedding)\n",
    "                    \n",
    "                if verbose:\n",
    "                    print(\"\\n\\n\")\n",
    "\n",
    "            if mode == \"train\":\n",
    "                print(\"\\n\\nepoch: %d; data batch-%d/%d; total average loss: %.6f\" % (epoch + 1, cond_list_idx + 1, total_data_batches, total_loss / num_iterations))\n",
    "            \n",
    "            if check_real_loss:\n",
    "                data_batch_loss = sample_gpt2(model, tokenizer, all_context_prompts, all_generated_prompt_length, t_pad_token, classifier, class_label)\n",
    "                epoch_loss += data_batch_loss\n",
    "                \n",
    "        if check_real_loss:\n",
    "            print(\"epoch %d: loss: %.6f\\n\\n\" % (epoch + 1, epoch_loss / total_data_batches))\n",
    "            print(\"<<<<<<>>>>>>\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_gpt2(model, tokenizer, all_context_prompts, all_generated_prompt_length, \n",
    "                t_pad_token, classifier, class_label):\n",
    "    \n",
    "    real_loss_all = 0\n",
    "    padding_token = tokenizer.encode(t_pad_token)[0]\n",
    "    \n",
    "    for real_i in range(len(all_context_prompts)):\n",
    "        cp_length_i = all_generated_prompt_length[real_i]\n",
    "        min_cp_length_i = torch.min(cp_length_i)  # tensor of float (no shape)\n",
    "        max_cp_length_i = torch.max(cp_length_i)\n",
    "        cp_i_text = all_context_prompts[real_i]\n",
    "        \n",
    "        cp_i_input_ids = list()\n",
    "        for cp_i_j in cp_i_text:\n",
    "            input_ids = tokenizer.encode(cp_i_j)\n",
    "            padding_len = max_cp_length_i - len(input_ids)\n",
    "            input_ids = input_ids + [padding_token] * padding_len\n",
    "            cp_i_input_ids.append(input_ids)\n",
    "        \n",
    "        # note: for instance, '\\n\\n' will be decoded as 628 in gpt2, but it may be encoded as '198 198' if it's connected to another token (e.g. '\\n\\n-'), which creates a problem in length\n",
    "        ck_max_cp_length_i = max(len(cp_i_inp) for cp_i_inp in cp_i_input_ids)\n",
    "        if ck_max_cp_length_i > max_cp_length_i:\n",
    "            max_cp_length_i = ck_max_cp_length_i\n",
    "            for cp_i_inp_idx in range(len(cp_i_input_ids)):\n",
    "                if len(cp_i_input_ids[cp_i_inp_idx]) < ck_max_cp_length_i:\n",
    "                    cp_i_input_ids[cp_i_inp_idx] += (max_cp_length_i - len(cp_i_input_ids[cp_i_inp_idx])) * [padding_token]\n",
    "                elif len(cp_i_input_ids[cp_i_inp_idx]) == ck_max_cp_length_i:\n",
    "                    cp_length_i[cp_i_inp_idx] = ck_max_cp_length_i\n",
    "        \n",
    "        cp_i_inputs = torch.tensor(cp_i_input_ids, dtype=torch.long, device=device)\n",
    "            \n",
    "        cp_i_batch_size = len(cp_i_text)\n",
    "        real_response_hidden = None\n",
    "        real_rp_h_ck = None\n",
    "        to_break = False\n",
    "        cp_i_response_not_done = torch.ones(cp_i_batch_size, 1, dtype=torch.uint8, device=device)\n",
    "        cp_i_generated_response_length = torch.zeros(cp_i_batch_size, 1, dtype=torch.long, device=device)\n",
    "        \n",
    "        with torch.no_grad():  # Need this?\n",
    "            cp_i_so_far = cp_i_inputs[:, :min_cp_length_i]  # bze x (min_cp_length_i - 1)\n",
    "            cp_i_context_lm_output = model(cp_i_inputs[:, :min_cp_length_i - 1])\n",
    "            past = cp_i_context_lm_output[\"past_key_values\"]\n",
    "            last = cp_i_inputs[:, min_cp_length_i - 1: min_cp_length_i]\n",
    "            \n",
    "            for rr_i in range(length):  # length + max_cp_length_i - min_cp_length_i?\n",
    "                lm_cp_i_output = model(last, past_key_values=past)\n",
    "                cp_i_logits, past, cp_i_hidden = (lm_cp_i_output[\"logits\"],  # bze, cur_seq_len, vocab_size\n",
    "                                                 lm_cp_i_output[\"past_key_values\"],  # acc_seq_len\n",
    "                                                 lm_cp_i_output[\"hidden_states\"])  # num_layers + 1, tuple of (bze, cur_seq_len, hid_sze)\n",
    "                                \n",
    "                cp_i_logits = penalize_new_line(cp_i_logits)\n",
    "                \n",
    "                last, _ = generate_next(cp_i_logits, cp_i_so_far, top_k=top_k, temperature=temperature, \n",
    "                                        repetition_penalty=repetition_penalty, sample=sample,\n",
    "                                        gumbel_softmax=False, gumbel_temperature=gumbel_temperature, detach=False)\n",
    "                \n",
    "                cp_i_last_hidden = cp_i_hidden[-1]\n",
    "                if real_response_hidden is None:\n",
    "                    real_response_hidden = cp_i_last_hidden\n",
    "                    real_rp_h_ck = last\n",
    "                else:\n",
    "                    real_response_hidden = torch.cat((real_response_hidden, cp_i_last_hidden), dim=1)  # bze, n, hid_size\n",
    "                    real_rp_h_ck = torch.cat((real_rp_h_ck, last), dim=1)  # bze, n  \n",
    "                if to_break:\n",
    "                    break\n",
    "                    \n",
    "                # manually assign end token if too long\n",
    "                if rr_i == length - 1:\n",
    "                    for rr_b_i in range(cp_i_batch_size):\n",
    "                        if cp_i_generated_response_length[rr_b_i] == 0:\n",
    "                            last[rr_b_i] = tokenizer.encode(stop_token)[0]  # encode outputs a list (1 element)\n",
    "            \n",
    "                # adjust\n",
    "                rr_i_is_generated = cp_length_i <= (rr_i + min_cp_length_i)  # bze x 1. is generated or stil in the context+prompt\n",
    "                rr_i_is_end_token = last == torch.tensor(tokenizer.encode(stop_token), device=device)  # bze x 1\n",
    "                rr_i_is_actually_ending = rr_i_is_generated * rr_i_is_end_token\n",
    "                \n",
    "                # keep track of response length\n",
    "                cp_i_generated_response_length = cp_i_generated_response_length + cp_i_response_not_done * rr_i_is_actually_ending * rr_i\n",
    "                \n",
    "                # if generated, use the generated token as last; otherwise (from context+prompt), copy the orignal token/gumbel_vector\n",
    "                if min_cp_length_i + rr_i < max_cp_length_i:\n",
    "                    last = last * rr_i_is_generated + cp_i_inputs[:, min_cp_length_i + rr_i].unsqueeze(1) * (~rr_i_is_generated)\n",
    "                else:\n",
    "                    last = last\n",
    "                \n",
    "                cp_i_so_far = torch.cat((cp_i_so_far, last), dim=1)\n",
    "                \n",
    "                cp_i_response_not_done = cp_i_response_not_done * (~rr_i_is_actually_ending)\n",
    "                if torch.sum(cp_i_response_not_done) == 0:\n",
    "                    to_break = True\n",
    "            \n",
    "            print(\"Real responses: \")\n",
    "            for r_pr_i in range(cp_i_batch_size):\n",
    "                if cp_i_generated_response_length[r_pr_i] == 0:\n",
    "                    print(tokenizer.decode(cp_i_so_far[r_pr_i, :].tolist()))  # exceeds the max length set (so generated_prompt_length is 0)\n",
    "                else:\n",
    "                    print(tokenizer.decode(cp_i_so_far[r_pr_i, :(cp_i_generated_response_length[r_pr_i].item() + 1 + min_cp_length_i)].tolist()))\n",
    "            print(\"***\" * 20)\n",
    "            \n",
    "            cp_i_extracted_hidden = None\n",
    "            for r_hb_i in range(cp_i_batch_size):\n",
    "                r_hb_i_start = cp_length_i[r_hb_i] - min_cp_length_i + 1\n",
    "                r_hb_i_end = cp_i_generated_response_length[r_hb_i] + 1 + 1\n",
    "                r_hb_i_hidden = torch.mean(real_response_hidden[r_hb_i:r_hb_i+1, r_hb_i_start:r_hb_i_end, :], dim=1)\n",
    "                if cp_i_extracted_hidden is None:\n",
    "                    cp_i_extracted_hidden = r_hb_i_hidden\n",
    "                else:\n",
    "                    cp_i_extracted_hidden = torch.cat((cp_i_extracted_hidden, r_hb_i_hidden), dim=0)\n",
    "            \n",
    "            prediction = classifier(cp_i_extracted_hidden)\n",
    "            label = torch.tensor([class_label], device=device, dtype=torch.long).repeat(cp_i_batch_size)\n",
    "            loss = ce_loss(prediction, label)\n",
    "            \n",
    "            real_loss_all += loss.item()\n",
    "                \n",
    "            print(\"loss: %.6f\" % loss.item())\n",
    "            print()\n",
    "            \n",
    "    print(\"all_loss\")\n",
    "    print(real_loss_all / len(all_context_prompts))\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    return real_loss_all / len(all_context_prompts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======training=======\n",
      "&&&&& epoch: 1 &&&&&\n",
      "=====Epoch: 1; data_batch: 1; Iteration: 1=====\n",
      "context + prompt\n",
      "<|endoftext|>My favorite music genre is death metal. In my mind, death metal is all about the heavy death metal.\n",
      "<|endoftext|>I listen to rap music. I love it.\n",
      "<|endoftext|>I workout four hours a day. When I get into the gym, I'm not going to lie, it's not a lot.\n",
      "<|endoftext|>I'm a christian. So I'm not going to pretend to know what the Bible is about, but I do know that if it's about Jesus as the Son of God, then I have a lot of respect for.\n",
      "************************************************************\n",
      "response: \n",
      " I love that they are constantly pushing their music forward and trying new things with different types of music.\n",
      " And I love it because it is so much more than rap music.\n",
      " I usually start in the morning, usually in my car, but I do it in my own apartment.\n",
      " And I believe.\n",
      "************************************************************\n",
      "discrim loss: 3.162913\n",
      "\n",
      "=======update loss: 3.162913=======\n",
      "\n",
      "\n",
      "\n",
      "=====Epoch: 1; data_batch: 1; Iteration: 2=====\n",
      "context + prompt\n",
      "<|endoftext|>My favorite music genre is death metal. The genre was born out of the need for something that would feel right in your stomach and not feel too weird, yet not feel too weird for you to listen to while eating.\n",
      "<|endoftext|>I listen to rap music. I don't listen to music as an artform, but to music as a medium.\n",
      "<|endoftext|>I workout four hours a day. I am an avid weightlifter.\n",
      "<|endoftext|>I'm a christian. I believe in Jesus and love Jesus and want him to be my king, my prophet, our savior.\n",
      "************************************************************\n",
      "response: \n",
      " Death metal is about being dead.\n",
      " I listen to rap music as a medium, because rap music is a unique form of music that has become popular in recent years.\n",
      " My goal is to achieve and maintain the same weight I had before I started my diet.\n",
      " But I know that he is not my Messiah; I know he doesn't even have a name.\n",
      "************************************************************\n",
      "discrim loss: 2.053444\n",
      "\n",
      "=======update loss: 2.053444=======\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch: 1; data batch-1/33; total average loss: 2.608179\n",
      "Real responses: \n",
      "<|endoftext|>My favorite music genre is death metal. In my mind, death metal is all about the heavy death metal. If you can't see it, you are missing it.\n",
      "<|endoftext|>I listen to rap music. I love it. It reminds me of the good times.\n",
      "<|endoftext|>I workout four hours a day. When I get into the gym, I'm not going to lie, it's not a lot. My heart rate is high, the adrenaline rush is high.\n",
      "<|endoftext|>I'm a christian. So I'm not going to pretend to know what the Bible is about, but I do know that if it's about Jesus as the Son of God, then I have a lot of respect for. I'm not.\n",
      "************************************************************\n",
      "loss: 2.998371\n",
      "\n",
      "Real responses: \n",
      "<|endoftext|>My favorite music genre is death metal. The genre was born out of the need for something that would feel right in your stomach and not feel too weird, yet not feel too weird for you to listen to while eating. In my opinion, metal can have the best music.\n",
      "<|endoftext|>I listen to rap music. I don't listen to music as an artform, but to music as a medium. It is my music.\n",
      "<|endoftext|>I workout four hours a day. I am an avid weightlifter. I have trained for almost 40 years.\n",
      "<|endoftext|>I'm a christian. I believe in Jesus and love Jesus and want him to be my king, my prophet, our savior. I am an atheist so when I was a kid my mom and dad were not only atheists but were also not allowed to have any religious.\n",
      "************************************************************\n",
      "loss: 2.293064\n",
      "\n",
      "all_loss\n",
      "2.645717144012451\n",
      "\n",
      "\n",
      "\n",
      "=====Epoch: 1; data_batch: 2; Iteration: 1=====\n",
      "context + prompt\n",
      "<|endoftext|>I like to dance. My favorite way to dance is to dance.\n",
      "<|endoftext|>I am married. My husband's name is Brian.\n",
      "<|endoftext|>I've one older sister. I was raised in a home that was a very small one, and she was my best friend and the only one who was able to help me.\n",
      "<|endoftext|>My favorite food is pizza with black olives. It's one of those things, I love when you make it.\n",
      "************************************************************\n",
      "response: \n",
      " It's the best way to make friends.\n",
      " He is a professional athlete with a background in track and field, golf, and swimming.\n",
      " She taught me to write, to write well, to write well at.\n",
      " It's a simple dish of tomatoes and mozzarella, some olive oil, some pepper, some salt, and some pepper sauce.\n",
      "************************************************************\n",
      "discrim loss: 5.165726\n",
      "\n",
      "=======update loss: 5.165726=======\n",
      "\n",
      "\n",
      "\n",
      "=====Epoch: 1; data_batch: 2; Iteration: 2=====\n",
      "context + prompt\n",
      "<|endoftext|>I like to dance. So when a friend of mine asked if I was going to get a tattoo of my friend's face I was all, \"Yeah! Let's do it! I want that!\" And I.\n",
      "<|endoftext|>I am married. I have two kids, a son, and a daughter.\n",
      "<|endoftext|>I've one older sister. She's been in a relationship and a couple years older.\n",
      "<|endoftext|>My favorite food is pizza with black olives. I've never had a slice of pizza without it.\n",
      "************************************************************\n",
      "response: \n",
      "am all, \"Yay!\" But then I thought.\n",
      " I love them dearly.\n",
      " She was in the military.\n",
      " I can eat anything without a slice of pizza.\n",
      "************************************************************\n",
      "discrim loss: 3.550882\n",
      "\n",
      "=======update loss: 3.550882=======\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch: 1; data batch-2/33; total average loss: 4.358304\n",
      "Real responses: \n",
      "<|endoftext|>I like to dance. My favorite way to dance is to dance. It's the most fun you ever have, because your mind doesn't wander, your body is not moving in weird ways, and your soul is not trying to be anything, it.\n",
      "<|endoftext|>I am married. My husband's name is Brian. We have a daughter named Charlotte who is five years old.\n",
      "<|endoftext|>I've one older sister. I was raised in a home that was a very small one, and she was my best friend and the only one who was able to help me. She would always help me with anything.\n",
      "<|endoftext|>My favorite food is pizza with black olives. It's one of those things, I love when you make it. When I was younger, I used to do that with my mom's pizza.\n",
      "************************************************************\n",
      "loss: 3.542053\n",
      "\n",
      "Real responses: \n",
      "<|endoftext|>I like to dance. So when a friend of mine asked if I was going to get a tattoo of my friend's face I was all, \"Yeah! Let's do it! I want that!\" And I. Am.\n",
      "<|endoftext|>I am married. I have two kids, a son, and a daughter. I don't have any problems.\n",
      "<|endoftext|>I've one older sister. She's been in a relationship and a couple years older. She's been married for 10 years and has three children.\n",
      "<|endoftext|>My favorite food is pizza with black olives. I've never had a slice of pizza without it. I can't get enough of it.\n",
      "************************************************************\n",
      "loss: 2.470909\n",
      "\n",
      "all_loss\n",
      "3.0064809322357178\n",
      "\n",
      "\n",
      "\n",
      "=====Epoch: 1; data_batch: 3; Iteration: 1=====\n",
      "context + prompt\n",
      "<|endoftext|>I like tacos. But when they're tacos, they're good.\n",
      "<|endoftext|>I'm four. This is a good time to be alive, but I think I should start by saying that there are still a few things I want to talk about.\n",
      "<|endoftext|>I also like to cook but i am not very good at it. I have always been an amateur chef.\n",
      "<|endoftext|>I own two vintage mustangs. I'm not sure I've ever owned a Mustang with a 4 door engine.\n",
      "************************************************************\n",
      "response: \n",
      " They're tasty.\n",
      " I've had a couple of weeks to reflect about what went wrong, what I did right,.\n",
      " So i tried everything I could find.\n",
      " I am now considering buying a new Mustang.\n",
      "************************************************************\n",
      "discrim loss: 3.080213\n",
      "\n",
      "=======update loss: 3.080213=======\n",
      "\n",
      "\n",
      "\n",
      "=====Epoch: 1; data_batch: 3; Iteration: 2=====\n",
      "context + prompt\n",
      "<|endoftext|>I like tacos. In fact, I've been to Mexico for two weeks and I've already made my first tortilla, which I ate with a side of salsa for dinner.\n",
      "<|endoftext|>I'm four. I'm five.\n",
      "<|endoftext|>I also like to cook but i am not very good at it. I have to go to the kitchen to cook.\n",
      "<|endoftext|>I own two vintage mustangs. I have a 1969 Mustang with a 9.\n",
      "************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response: \n",
      " I love tortillas and am so excited to be back.\n",
      " I'm six.\n",
      " I know what i like to cook and I am not good at it.\n",
      "6\" wheels and a 5-speed automatic.\n",
      "************************************************************\n",
      "discrim loss: 2.227957\n",
      "\n",
      "=======update loss: 2.227957=======\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch: 1; data batch-3/33; total average loss: 2.654085\n",
      "Real responses: \n",
      "<|endoftext|>I like tacos. But when they're tacos, they're good. And sometimes you need something a little different.\n",
      "<|endoftext|>I'm four. This is a good time to be alive, but I think I should start by saying that there are still a few things I want to talk about. I'll leave the rest of the story up to you to decide.\n",
      "<|endoftext|>I also like to cook but i am not very good at it. I have always been an amateur chef. I am not even good at making anything.\n",
      "<|endoftext|>I own two vintage mustangs. I'm not sure I've ever owned a Mustang with a 4 door engine. I've owned many Mustangs over the years, but this was my first and only.\n",
      "************************************************************\n",
      "loss: 2.840098\n",
      "\n",
      "Real responses: \n",
      "<|endoftext|>I like tacos. In fact, I've been to Mexico for two weeks and I've already made my first tortilla, which I ate with a side of salsa for dinner. But I'm always looking to try new things, like.\n",
      "<|endoftext|>I'm four. I'm five. I'm six.\n",
      "<|endoftext|>I also like to cook but i am not very good at it. I have to go to the kitchen to cook. I can get through some of the meals but not a lot of dishes and if i can't get through them i.\n",
      "<|endoftext|>I own two vintage mustangs. I have a 1969 Mustang with a 9.25\" bore.\n",
      "************************************************************\n",
      "loss: 1.779605\n",
      "\n",
      "all_loss\n",
      "2.30985164642334\n",
      "\n",
      "\n",
      "\n",
      "=====Epoch: 1; data_batch: 4; Iteration: 1=====\n",
      "context + prompt\n",
      "<|endoftext|>I try various coffees as a hobby. I like to brew with a couple of different coffees and try various combinations of flavors.\n",
      "<|endoftext|>I've a pet cow. I am very happy with it.\n",
      "<|endoftext|>I work at a nursing home. We do not accept people who are HIV positive and are in need of treatment.\n",
      "<|endoftext|>Hey my name is larry and i am a chef. i am trying to make a sandwich, my idea was simple, put the chicken into a hot water bath to make the breadcrumbs, fry the bread and then.\n",
      "************************************************************\n",
      "response: \n",
      " This is one of the most interesting and delicious blends I have tried in the past few months.\n",
      " It's a lovely creature.\n",
      " If you are HIV negative and need help getting out of your housing situation, please call my office at 616.\n",
      " the breadcrumbs.\n",
      "************************************************************\n",
      "discrim loss: 4.458238\n",
      "\n",
      "=======update loss: 4.458238=======\n",
      "\n",
      "\n",
      "\n",
      "=====Epoch: 1; data_batch: 4; Iteration: 2=====\n",
      "context + prompt\n",
      "<|endoftext|>I try various coffees as a hobby. I'm a little obsessed with coffee and I'm trying different recipes and recipes.\n",
      "<|endoftext|>I've a pet cow. Her name is Momma, and she loves eating my milk.\n",
      "<|endoftext|>I work at a nursing home. I have a great friend who is a wonderful person, and we are close.\n",
      "<|endoftext|>Hey my name is larry and i am a chef. i make delicious things.\n",
      "************************************************************\n",
      "response: \n",
      " Some of my favorites include: The Roasted Ethiopian Coffee with Ginger & Lemon, a Coffee with Honey, the Roasted Ethiopian Coffee with Chocolate and a very.\n",
      " She's a little bit older than I am, but she still loves me, so I think she's doing fine with my milk.\n",
      " We have a great job, a fantastic home, and a wonderful family.\n",
      " i make things that are good for the environment, good for health, good for our community.\n",
      "************************************************************\n",
      "discrim loss: 6.436638\n",
      "\n",
      "=======update loss: 6.436638=======\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch: 1; data batch-4/33; total average loss: 5.447438\n",
      "Real responses: \n",
      "<|endoftext|>I try various coffees as a hobby. I like to brew with a couple of different coffees and try various combinations of flavors. I'm always looking for ways to improve my taste buds, I'm also passionate about coffee! In this post, we will.\n",
      "<|endoftext|>I've a pet cow. I am very happy with it. It is a beautiful cow with an enormous head and huge eyes.\n",
      "<|endoftext|>I work at a nursing home. We do not accept people who are HIV positive and are in need of treatment. I'm not surprised by this fact, because we have a long way to go.\n",
      "<|endoftext|>Hey my name is larry and i am a chef. i am trying to make a sandwich, my idea was simple, put the chicken into a hot water bath to make the breadcrumbs, fry the bread and then. cook the chicken, i was.\n",
      "************************************************************\n",
      "loss: 4.454193\n",
      "\n",
      "Real responses: \n",
      "<|endoftext|>I try various coffees as a hobby. I'm a little obsessed with coffee and I'm trying different recipes and recipes. Some are simple, some are complex and some are just plain delicious.\n",
      "<|endoftext|>I've a pet cow. Her name is Momma, and she loves eating my milk. I don't mind it.\n",
      "<|endoftext|>I work at a nursing home. I have a great friend who is a wonderful person, and we are close. She was an excellent nurse, and a friend and confidant to me during my time at my nursing home.\n",
      "<|endoftext|>Hey my name is larry and i am a chef. i make delicious things. i make them in my kitchen and i make them for my friends who live in the area.\n",
      "************************************************************\n",
      "loss: 4.037606\n",
      "\n",
      "all_loss\n",
      "4.245899438858032\n",
      "\n",
      "\n",
      "\n",
      "=====Epoch: 1; data_batch: 5; Iteration: 1=====\n",
      "context + prompt\n",
      "<|endoftext|>I am an elementary school teacher. The teacher is very busy and I have trouble keeping myself focused, but the kids are doing their best.\n",
      "<|endoftext|>I live in a house. I'm not a fan of the bathroom.\n",
      "<|endoftext|>I'm high maintenance. I like to do everything at once, but I don't usually do all my own work.\n",
      "<|endoftext|>I spent a decade working in the human services field. I've been there for almost 15 years as a human resources manager.\n",
      "************************************************************\n",
      "response: \n",
      " We're trying to make sure everyone has fun, but we also try to get the kids interested in the subject, and I think.\n",
      " I have to pee every day for a year, or else get sick.\n",
      " I have to do everything by myself and I can't be a team player, but when the team is doing well, I'm the last person to.\n",
      " In fact, I started my career at a major government agency.\n",
      "************************************************************\n",
      "discrim loss: 2.408236\n",
      "\n",
      "=======update loss: 2.408236=======\n",
      "\n",
      "\n",
      "\n",
      "=====Epoch: 1; data_batch: 5; Iteration: 2=====\n",
      "context + prompt\n",
      "<|endoftext|>I am an elementary school teacher. I have been teaching English in a district that is located in the northern part of the state and I have taught a lot of English students, mostly middle schoolers.\n",
      "<|endoftext|>I live in a house. There is a lot of furniture around the house, but no furniture is really needed for it.\n",
      "<|endoftext|>I'm high maintenance. I want to have a great time in this town and I want to meet all the people I want to meet.\n",
      "<|endoftext|>I spent a decade working in the human services field. I was responsible for the creation and management of all human resources departments, and the hiring and training of employees, managers, etc.\n",
      "************************************************************\n",
      "response: \n",
      " I have always been very passionate about teaching English and I have always loved teaching in a class setting.\n",
      " I have no problem with having a kitchen, living room, or dining room where I can just eat whatever I want and then take off to bed, but I do not want to have a large.\n",
      " I want to do things I've never done before.\n",
      " The most important aspect of human resources is its structure and structure is important to success because it allows the department leaders to plan and manage their.\n",
      "************************************************************\n",
      "discrim loss: 3.731198\n",
      "\n",
      "=======update loss: 3.731198=======\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch: 1; data batch-5/33; total average loss: 3.069717\n",
      "Real responses: \n",
      "<|endoftext|>I am an elementary school teacher. The teacher is very busy and I have trouble keeping myself focused, but the kids are doing their best. I love to make them happy, teach them about the world, and make them feel comfortable and accepted.\n",
      "<|endoftext|>I live in a house. I'm not a fan of the bathroom. It's too narrow and cramped and I'm always having to move around my things, and the whole thing is just too loud and uncomfortable.\n",
      "<|endoftext|>I'm high maintenance. I like to do everything at once, but I don't usually do all my own work. This week, I've been doing a lot of reading.\n",
      "<|endoftext|>I spent a decade working in the human services field. I've been there for almost 15 years as a human resources manager. In that time, I've worked with clients, including the Department of Homeland Security, the U.\n",
      "************************************************************\n",
      "loss: 4.055960\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real responses: \n",
      "<|endoftext|>I am an elementary school teacher. I have been teaching English in a district that is located in the northern part of the state and I have taught a lot of English students, mostly middle schoolers. My students are mostly from the south of the state, and most of them speak English as their native language.\n",
      "<|endoftext|>I live in a house. There is a lot of furniture around the house, but no furniture is really needed for it. The furniture is a nice addition to the space, and I'm happy with what it is.\n",
      "<|endoftext|>I'm high maintenance. I want to have a great time in this town and I want to meet all the people I want to meet. I like to do things on my own and it seems like every single person there has a different style of dress and hair, even the girls have different hairstyles.\n",
      "<|endoftext|>I spent a decade working in the human services field. I was responsible for the creation and management of all human resources departments, and the hiring and training of employees, managers, etc. I am a former director of Human Resources.\n",
      "************************************************************\n",
      "loss: 3.108015\n",
      "\n",
      "all_loss\n",
      "3.5819873809814453\n",
      "\n",
      "\n",
      "\n",
      "=====Epoch: 1; data_batch: 6; Iteration: 1=====\n",
      "context + prompt\n",
      "<|endoftext|>I'm a professional tennis player. I've always played tennis in New Jersey.\n",
      "<|endoftext|>My bedroom is purple and lime green. It's so warm, the room is like a tropical beach for my dog, the walls are covered with tropical fruit, and the furniture looks like it was made by the designer of.\n",
      "<|endoftext|>I've a boyfriend. We used to hang out.\n",
      "<|endoftext|>I move south during the winter. I don't have a lot of money left and can't get into work.\n",
      "************************************************************\n",
      "response: \n",
      " I'm a great fan, but I'm not the biggest fan of the New Jersey Devils.\n",
      " There's nothing else I.\n",
      " He was a bit of a snob.\n",
      " I don't have anything to look for.\n",
      "************************************************************\n",
      "discrim loss: 1.857148\n",
      "\n",
      "=======update loss: 1.857148=======\n",
      "\n",
      "\n",
      "\n",
      "=====Epoch: 1; data_batch: 6; Iteration: 2=====\n",
      "context + prompt\n",
      "<|endoftext|>I'm a professional tennis player. And I think it's important that we talk about women's issues, especially when it comes to the issues around gender and the sport.\n",
      "<|endoftext|>My bedroom is purple and lime green. I have an open floor plan.\n",
      "<|endoftext|>I've a boyfriend. We were together for 2 years and I think it's because he wanted to do me a favor and take me to his house on a Saturday evening so that I could do my job (which is.\n",
      "<|endoftext|>I move south during the winter. The wind is blowing at 20 mph, but I'm wearing a hat.\n",
      "************************************************************\n",
      "response: \n",
      " I've always been attracted to tennis.\n",
      " I have two large closet doors.\n",
      " He does it so much better than my job.\n",
      " I am wearing a helmet, but I'm not wearing my glasses, nor have I been hit by a car while driving and don't want my helmet to shatter.\n",
      "************************************************************\n",
      "discrim loss: 2.861779\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-c393d5384d4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m                          \u001b[0mrepetition_penalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madam_epsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_pad_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                          \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_cond_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_real_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                          \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                         )\n\u001b[1;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\\n\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-b8162a8e8a27>\u001b[0m in \u001b[0;36mgenerate_prompt_response\u001b[0;34m(model, tokenizer, mode, lm_bos_output, batch_size, device, class_label, num_iterations, learning_rate, gradient_accumulation_steps, sample, gumbel_softmax, detach, reset_pos_emb, not_mask_trigger, gumbel_temperature, top_k, temperature, repetition_penalty, adam_epsilon, max_grad_norm, length, t_pad_token, stop_token, num_epochs, context_list, verbose, check_real_loss, seed)\u001b[0m\n\u001b[1;32m    310\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnum_of_triggers\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                     \u001b[0;31m# compute gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                     \u001b[0mdiscrim_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                     \u001b[0;31m# # debugging: check grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/zero/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/zero/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# run\n",
    "if not multiple_input:\n",
    "    cond_list = [context_text] * batch_size\n",
    "else:\n",
    "    train_cond_list = read_file(train_filename)\n",
    "    eval_cond_list = read_file(eval_filename)\n",
    "\n",
    "eval_num_per_cond = 2\n",
    "\n",
    "# baseline:\n",
    "# print(\"=======getting baselines=======\")\n",
    "# generate_prompt_response(model, tokenizer, \"eval\", lm_bos_output, batch_size, device, class_label,\n",
    "#                          eval_num_per_cond, 0, gradient_accumulation_steps, sample, False,\n",
    "#                          False, reset_pos_emb, not_mask_trigger, gumbel_temperature, top_k, temperature, \n",
    "#                          repetition_penalty, adam_epsilon, max_grad_norm, length, t_pad_token, stop_token,\n",
    "#                          1, eval_cond_list, verbose, True,\n",
    "#                          seed,\n",
    "#                         )\n",
    "# print(\"\\n\\n\\n\")\n",
    "    \n",
    "# train:\n",
    "print(\"=======training=======\")\n",
    "generate_prompt_response(model, tokenizer, \"train\", lm_bos_output, batch_size, device, class_label,\n",
    "                         num_iterations, learning_rate, gradient_accumulation_steps, sample, gumbel_softmax,\n",
    "                         detach, reset_pos_emb, not_mask_trigger, gumbel_temperature, top_k, temperature, \n",
    "                         repetition_penalty, adam_epsilon, max_grad_norm, length, t_pad_token, stop_token,\n",
    "                         num_epochs, train_cond_list, verbose, check_real_loss,\n",
    "                         seed,\n",
    "                        )\n",
    "print(\"\\n\\n\\n\")\n",
    "\n",
    "# eval:\n",
    "print(\"=======evaluation=======\")\n",
    "generate_prompt_response(model, tokenizer, \"eval\", lm_bos_output, batch_size, device, class_label,\n",
    "                         eval_num_per_cond, 0, gradient_accumulation_steps, sample, False,\n",
    "                         False, reset_pos_emb, not_mask_trigger, gumbel_temperature, top_k, temperature, \n",
    "                         repetition_penalty, adam_epsilon, max_grad_norm, length, t_pad_token, stop_token,\n",
    "                         1, eval_cond_list, verbose, True,\n",
    "                         seed,\n",
    "                        )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zero",
   "language": "python",
   "name": "zero"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
