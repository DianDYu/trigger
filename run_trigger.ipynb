{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "from typing import List, Optional, Tuple, Union\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "from utils import get_classifier, generate_next, concat_past, expand_past\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "device = \"cuda\"\n",
    "\n",
    "pretrained_model = \"gpt2-medium\"\n",
    "discrim = \"sentiment\"\n",
    "class_label = 3\n",
    "num_of_triggers = 1\n",
    "\n",
    "trigger_format = \"key_value\"  # token or key_value\n",
    "TRIGGER_POSITION_ID = 0  # for position reset\n",
    "\n",
    "num_iterations = 2\n",
    "learning_rate = 5e-3\n",
    "# learning_rate = 0\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "sample = True\n",
    "gumbel_softmax = True\n",
    "detach = True\n",
    "# ----------------------\n",
    "reset_pos_emb = True\n",
    "not_mask_trigger = False\n",
    "gumbel_temperature = 1.0\n",
    "\n",
    "batch_size = 4\n",
    "multiple_input = True\n",
    "\n",
    "# more or less fixed for generation\n",
    "top_k = 10\n",
    "temperature = 1.0\n",
    "repetition_penalty = 1.0\n",
    "adam_epsilon = 1e-8\n",
    "max_grad_norm = 1.0\n",
    "length = 40\n",
    "\n",
    "if detach and not gumbel_softmax:\n",
    "    assert False, \"require gumbel softmax when using detach\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# load pretrained model\n",
    "model = GPT2LMHeadModel.from_pretrained(pretrained_model, output_hidden_states=True)\n",
    "model.to(device)\n",
    "model.eval()  # do not need batchnorm or dropout layers for training/eval\n",
    "# ori_model.eval()\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model)\n",
    "\n",
    "# Freeze GPT-2 weights\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "classifier, class_id = get_classifier(discrim, class_label, device)\n",
    "\n",
    "num_layers = model.config.n_layer\n",
    "\n",
    "ce_loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cond_text_list = [\"This is a terrible restaurant.\", \"I like drinking water.\", \"It is raining today.\", \"I'm doing my homework.\"]\n",
    "cond_text_list = [\"My favorite music genre is death metal.\", \"I listen to rap music.\", \"I workout four hours a day.\", \"I'm a christian.\"]\n",
    "# cond_text_list = [\"I don't like this restaurant.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if multiple_input:\n",
    "    # TODO: implement dataloader to get a batch of cond_text\n",
    "    batch_max_length = 0\n",
    "    batch_min_length = 10000\n",
    "    batch_input_ids = list()\n",
    "    all_inputs, all_attention_masks, all_lengths, all_padding_length = list(), list(), list(), list()\n",
    "    padding_token = tokenizer.encode(tokenizer.bos_token)[0]  # WARNING: BOS is for GPT2 only. Should use padding token\n",
    "    for cond_text in cond_text_list[:batch_size]:\n",
    "        inputs_ids = tokenizer.encode(tokenizer.bos_token + cond_text)\n",
    "        batch_max_length = len(inputs_ids) if len(inputs_ids) > batch_max_length else batch_max_length\n",
    "        batch_min_length = len(inputs_ids) if len(inputs_ids) < batch_min_length else batch_min_length\n",
    "        batch_input_ids.append(inputs_ids)\n",
    "    for inputs_ids in batch_input_ids:\n",
    "        all_lengths.append(len(inputs_ids))\n",
    "        padding_len = batch_max_length - len(inputs_ids)\n",
    "#         attention_mask = [0] * padding_len + [1] * len(inputs_ids)\n",
    "#         inputs_ids = [padding_token] * padding_len + inputs_ids\n",
    "        attention_mask = [1] * len(inputs_ids) + [0] * padding_len\n",
    "        inputs_ids = inputs_ids + [padding_token] * padding_len \n",
    "        \n",
    "        all_padding_length.append(padding_len)\n",
    "        all_inputs.append(inputs_ids)\n",
    "        all_attention_masks.append(attention_mask)\n",
    "        \n",
    "    all_input_ids = torch.tensor(all_inputs, dtype=torch.long, device=device)\n",
    "    all_attention_masks = torch.tensor(all_attention_masks, dtype=torch.long, device=device)\n",
    "        \n",
    "else:    \n",
    "    cond_text = \"Today is Monday.\"\n",
    "    tokenized_cond_text = tokenizer.encode(tokenizer.bos_token + cond_text)\n",
    "\n",
    "    context_t = torch.tensor(tokenized_cond_text, device=device, dtype=torch.long)\n",
    "    while len(context_t.shape) < 2:\n",
    "        context_t = context_t.unsqueeze(0)\n",
    "    input_length = context_t.shape[-1]\n",
    "    all_input_ids = context_t.repeat(batch_size, 1).to(device)\n",
    "    all_attention_masks = torch.ones(batch_size, input_length).to(torch.long).to(device)\n",
    "    all_padding_length = [0] * batch_size\n",
    "    all_lengths = [input_length] * batch_size\n",
    "    batch_min_length = input_length\n",
    "    batch_max_length = input_length\n",
    "\n",
    "lm_bos_output = model(torch.tensor(tokenizer.encode(tokenizer.bos_token), dtype=torch.long, device=device).unsqueeze(0).repeat(batch_size, 1))  # BOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: For GPT2 only!!!\n",
    "new_line_idx = 198  # '\\n'\n",
    "def penalize_new_line(logits):\n",
    "    for b_i in range(logits.shape[0]):\n",
    "        if logits[b_i, -1, new_line_idx] < 0:\n",
    "            logits[b_i, -1, new_line_idx] *= 5\n",
    "        else:\n",
    "            logits[b_i, -1, new_line_idx] /= 5\n",
    "#     if logits[:, -1, new_line_idx] < 0:  # note: cannot do this since there are multiple values for boolean\n",
    "#         logits[:, -1, new_line_idx] *= 5\n",
    "#     else:\n",
    "#         logits[:, -1, new_line_idx] /= 5\n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize trigger\n",
    "# Note: since we use the same trigger for all inputs in a batch, we only create/register trigger(s) for one and repeat it\n",
    "if num_of_triggers > 0:\n",
    "    if trigger_format == \"token\":  # learn a continuous embedding\n",
    "        trigger_embedding_list = []\n",
    "        for _ in range(num_of_triggers):\n",
    "            trigger_embedding_i = copy.deepcopy(model.transformer.wte(\n",
    "                torch.tensor(tokenizer.encode(tokenizer.bos_token), device=device, dtype=torch.long).unsqueeze(0)))\n",
    "#             trigger_embedding_i.requires_grad = True\n",
    "            trigger_embedding_list.append(trigger_embedding_i)\n",
    "        ori_trigger_embedding = nn.Parameter(torch.cat(trigger_embedding_list, dim=1))  # bze x n x emb_size\n",
    "        model.ori_trigger_embedding = ori_trigger_embedding  # register to the model (optimizer)\n",
    "#         trigger_embedding = trigger_embedding.repeat(batch_size, 1, 1)  # cannot do it here, otherwise trigger_embedding becomes a non-leaf node where the grad will not backprop\n",
    "    elif trigger_format == \"key_value\":  # learn key values\n",
    "        ori_trigger_key_values = [(None, None) for _ in range(num_layers)]\n",
    "        bos_key_values = model(torch.tensor(tokenizer.encode(tokenizer.bos_token), dtype=torch.long).unsqueeze(0).to(device))[\n",
    "                        \"past_key_values\"]\n",
    "        for layer in range(num_layers):\n",
    "            for i_t in range(num_of_triggers):\n",
    "                trigger_i_key_value = copy.deepcopy(bos_key_values)\n",
    "                # key, value shape: bze, num_heads, seq_len, embed_per_head\n",
    "                trigger_i_key, trigger_i_value = nn.Parameter(trigger_i_key_value[layer][0]), \\\n",
    "                                                 nn.Parameter(trigger_i_key_value[layer][1])\n",
    "\n",
    "                trigger_i_key.requires_grad = True\n",
    "                trigger_i_value.requires_grad = True\n",
    "\n",
    "                if ori_trigger_key_values[layer][0] is None:\n",
    "                    ori_trigger_key_values[layer] = (trigger_i_key, trigger_i_value)\n",
    "                else:\n",
    "                    # if multiple triggers\n",
    "                    trigger_key = nn.Parameter(torch.cat((ori_trigger_key_values[layer][0], trigger_i_key), dim=-2))\n",
    "                    trigger_value = nn.Parameter(torch.cat((ori_trigger_key_values[layer][1], trigger_i_value), dim=-2))\n",
    "                    ori_trigger_key_values[layer] = (trigger_key, trigger_value)\n",
    "\n",
    "            # register parameter into optimizer\n",
    "            key_name = \"l_%d_key\" % layer\n",
    "            value_name = \"l_%d_value\" % layer\n",
    "            if num_of_triggers == 1:\n",
    "                model.register_parameter(name=key_name, param=trigger_i_key)\n",
    "                model.register_parameter(name=value_name, param=trigger_i_value)\n",
    "            else:\n",
    "                model.register_parameter(name=key_name, param=trigger_key)\n",
    "                model.register_parameter(name=value_name, param=trigger_value)\n",
    "\n",
    "        ori_trigger_key_values = tuple(ori_trigger_key_values)\n",
    "        model.ori_trigger_key_values = ori_trigger_key_values\n",
    "#         trigger_key_values = expand_past(trigger_key_values, num_layers, batch_size)  # similar to trigger_embedding, need leaf level grad\n",
    "    else:\n",
    "        assert False, \"trigger_format: %s not supported\" % trigger_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimizing params: \n",
      "l_0_key l_0_value l_1_key l_1_value l_2_key l_2_value l_3_key l_3_value l_4_key l_4_value l_5_key l_5_value l_6_key l_6_value l_7_key l_7_value l_8_key l_8_value l_9_key l_9_value l_10_key l_10_value l_11_key l_11_value l_12_key l_12_value l_13_key l_13_value l_14_key l_14_value l_15_key l_15_value l_16_key l_16_value l_17_key l_17_value l_18_key l_18_value l_19_key l_19_value l_20_key l_20_value l_21_key l_21_value l_22_key l_22_value l_23_key l_23_value\n"
     ]
    }
   ],
   "source": [
    "param_optimizer = list(filter(lambda p: p[1].requires_grad, list(model.named_parameters())))\n",
    "\n",
    "# debugging: get all optimized param names\n",
    "print(\"optimizing params: \")\n",
    "print(\" \".join(o[0] for o in param_optimizer))\n",
    "\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "        'weight_decay': 0.0,\n",
    "    },\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                  lr=learning_rate,\n",
    "                  eps=adam_epsilon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Iteration: 1=====\n",
      "context + prompt\n",
      "<|endoftext|>My favorite music genre is death metal. In my mind, death metal is all about the heavy death metal.\n",
      "<|endoftext|>I listen to rap music. I love it.\n",
      "<|endoftext|>I workout four hours a day. When I get into the gym, I'm not going to lie, it's not a lot.\n",
      "<|endoftext|>I'm a christian. So I'm not going to pretend to know what the Bible is about, but I do know that if it's about Jesus as the Son of God, then I have a lot of respect for.\n",
      "************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dianyu/anaconda3/envs/zero/lib/python3.6/site-packages/ipykernel_launcher.py:370: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response: \n",
      " I love that they are constantly pushing their music forward and trying new things with different types of music.\n",
      " And I love it because it is so much more than rap music.\n",
      " I usually start in the morning, usually in my car, but I do it in my own apartment.\n",
      " And I believe.\n",
      "************************************************************\n",
      "discrim loss: 3.162913\n",
      "\n",
      "=======update loss: 3.162913=======\n",
      "((Parameter containing:\n",
      "tensor([[[[ 0.3533,  0.1094,  0.3349,  ..., -0.6093,  0.1013, -0.1574]],\n",
      "\n",
      "         [[-0.4688, -0.8916, -0.9162,  ...,  0.1191,  0.4493,  0.5001]],\n",
      "\n",
      "         [[-0.4549, -0.5976, -0.3841,  ...,  0.5832, -0.0510, -0.0087]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.3807, -0.3904, -0.0723,  ...,  0.2100,  0.4539, -0.5294]],\n",
      "\n",
      "         [[ 0.7991, -0.8414, -0.3831,  ...,  0.0178, -1.1266,  0.4451]],\n",
      "\n",
      "         [[ 0.3184,  0.4240, -0.3132,  ...,  0.6935, -1.6754, -1.0158]]]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[[[ 0.0305, -0.0360, -0.0099,  ..., -0.0295,  0.0309,  0.0283]],\n",
      "\n",
      "         [[ 0.0093,  0.0095,  0.0028,  ...,  0.0453,  0.0038,  0.0110]],\n",
      "\n",
      "         [[ 0.0006,  0.0313, -0.0193,  ...,  0.0270, -0.0374, -0.0533]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0232, -0.0683,  0.0212,  ...,  0.0674,  0.0319, -0.0171]],\n",
      "\n",
      "         [[-0.0158, -0.0081, -0.0016,  ...,  0.0145,  0.0141,  0.0006]],\n",
      "\n",
      "         [[ 0.0723, -0.0179,  0.0162,  ...,  0.0269, -0.0392, -0.0035]]]],\n",
      "       device='cuda:0', requires_grad=True)), (Parameter containing:\n",
      "tensor([[[[ 0.1439,  1.0208, -0.2035,  ...,  0.6651, -2.0378,  0.1591]],\n",
      "\n",
      "         [[-1.8185, -0.1824,  3.2277,  ..., -0.2863,  0.4398, -0.2430]],\n",
      "\n",
      "         [[ 1.4692,  1.3983,  1.6473,  ...,  2.1470,  0.2062, -1.0760]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.7686, -0.3066,  0.6745,  ...,  0.1411, -0.4838,  0.6724]],\n",
      "\n",
      "         [[-0.5110,  0.3308, -0.2442,  ...,  0.1138, -0.0582, -0.6036]],\n",
      "\n",
      "         [[ 0.0375, -0.5854, -0.4479,  ...,  0.6607,  3.9235,  0.2650]]]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[[[ 6.4705e-02,  5.4950e-02,  3.9059e-01,  ..., -1.6512e-02,\n",
      "            2.0104e-01, -1.4521e-01]],\n",
      "\n",
      "         [[ 2.8426e-01, -6.9617e-01, -9.5134e-02,  ...,  1.7617e-01,\n",
      "            3.1766e-01,  4.6075e-02]],\n",
      "\n",
      "         [[-3.7091e-01,  1.9623e-01, -1.3829e-01,  ..., -6.3363e-02,\n",
      "            1.7595e-01,  2.3491e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.1502e-01,  2.1290e-01,  1.2719e-01,  ..., -4.1381e-01,\n",
      "           -3.2537e-01, -7.1268e-01]],\n",
      "\n",
      "         [[ 1.5249e-01,  2.4353e-01, -2.8757e-01,  ...,  2.1598e-01,\n",
      "            1.6663e-01,  3.8359e-04]],\n",
      "\n",
      "         [[-7.2312e-02, -4.9616e-02,  3.1042e-02,  ..., -4.2386e-01,\n",
      "            4.3239e-01, -1.4066e-01]]]], device='cuda:0', requires_grad=True)), (Parameter containing:\n",
      "tensor([[[[ 0.1714, -0.3304, -0.1503,  ..., -0.1386,  0.1255, -0.0698]],\n",
      "\n",
      "         [[ 0.2890, -0.2518, -0.0258,  ..., -0.8224, -2.1591, -0.0380]],\n",
      "\n",
      "         [[ 0.1219,  0.9754, -0.4228,  ..., -0.7927, -0.5103, -0.2178]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.8614, -0.1367, -0.2852,  ...,  0.6592,  0.0138,  0.6175]],\n",
      "\n",
      "         [[ 0.9817, -1.2999, -1.1292,  ...,  0.7479,  0.0853, -1.5561]],\n",
      "\n",
      "         [[ 1.6637,  0.6895,  0.7824,  ..., -0.0839, -0.1511, -0.2820]]]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[[[ 0.2876, -0.4162,  0.2465,  ...,  0.7477,  0.0722,  0.2091]],\n",
      "\n",
      "         [[-0.1199,  0.2438,  0.1705,  ..., -0.0499,  0.0696,  0.0763]],\n",
      "\n",
      "         [[ 0.1285, -1.4394,  0.2783,  ..., -0.3757,  0.4053,  0.4908]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.2148, -0.0353, -0.0615,  ..., -0.0209,  0.1561, -0.0687]],\n",
      "\n",
      "         [[ 0.3517,  0.3943,  1.0229,  ...,  0.1149,  0.0493,  0.1401]],\n",
      "\n",
      "         [[-0.3376, -0.0960, -0.2176,  ...,  0.0716,  0.0136, -0.1476]]]],\n",
      "       device='cuda:0', requires_grad=True)), (Parameter containing:\n",
      "tensor([[[[-0.5842, -1.1782,  0.1832,  ...,  0.9703, -0.1956, -0.1250]],\n",
      "\n",
      "         [[-0.3163, -0.0897, -0.4997,  ..., -0.4079,  0.2050, -1.5145]],\n",
      "\n",
      "         [[-0.1149, -0.0350,  0.2190,  ..., -0.1874,  0.4454,  0.5011]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.3103, -0.8687, -0.0196,  ..., -1.0733,  1.0728,  0.8270]],\n",
      "\n",
      "         [[ 0.6141,  0.5499, -0.5343,  ...,  0.2073,  0.4506,  0.7047]],\n",
      "\n",
      "         [[ 0.8941, -1.0133, -0.0757,  ..., -1.0426, -0.0061,  0.8916]]]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[[[-0.0371,  0.0253,  0.0580,  ..., -0.3297,  0.1297, -0.0045]],\n",
      "\n",
      "         [[-0.0342,  0.0207,  0.0751,  ...,  0.0810, -0.0396,  0.0719]],\n",
      "\n",
      "         [[-0.0251, -0.5523,  0.0396,  ..., -0.0806,  0.0113,  0.0686]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0269, -0.0310,  0.0598,  ..., -0.0039, -0.0392,  0.1663]],\n",
      "\n",
      "         [[ 0.1181,  0.0420, -0.0497,  ...,  0.0820, -0.2476, -0.0364]],\n",
      "\n",
      "         [[ 0.0266, -0.0381, -0.1915,  ...,  0.1082,  0.0584, -0.0936]]]],\n",
      "       device='cuda:0', requires_grad=True)), (Parameter containing:\n",
      "tensor([[[[-0.6166,  1.7424,  0.0981,  ...,  0.3045, -1.5169, -0.4743]],\n",
      "\n",
      "         [[-1.1888, -1.4755, -0.0457,  ..., -0.2795, -0.3613,  1.4449]],\n",
      "\n",
      "         [[-0.0040,  0.7010,  0.1437,  ...,  1.7621, -0.0336, -0.1918]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.5645,  3.7268,  0.0396,  ...,  2.5535,  3.1043, -3.0420]],\n",
      "\n",
      "         [[ 1.0861, -1.2307, -0.4519,  ...,  0.2201,  1.3642, -1.0745]],\n",
      "\n",
      "         [[ 0.4473, -0.2435, -0.0416,  ...,  0.2021,  0.1138, -0.3671]]]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[[[-0.0467, -0.0641, -0.0457,  ...,  0.1306, -0.0486, -0.0517]],\n",
      "\n",
      "         [[ 0.2694,  0.0322, -0.1285,  ...,  0.0039, -0.9217, -0.0656]],\n",
      "\n",
      "         [[ 0.0026, -0.0763,  0.0323,  ..., -0.0677, -0.0292,  0.0300]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0058,  0.0223,  0.0565,  ...,  0.0857,  0.1564,  0.1726]],\n",
      "\n",
      "         [[-0.0258, -0.0438, -0.0353,  ..., -0.0682,  0.0891,  0.0395]],\n",
      "\n",
      "         [[-0.0646,  0.1056,  0.0132,  ...,  0.0196, -0.0529, -0.0899]]]],\n",
      "       device='cuda:0', requires_grad=True)), (Parameter containing:\n",
      "tensor([[[[-0.3722,  0.8223,  0.2171,  ..., -0.5442, -0.0895,  1.0517]],\n",
      "\n",
      "         [[-1.0791,  1.3715,  0.6256,  ...,  0.6238, -0.1513,  0.4709]],\n",
      "\n",
      "         [[-0.1824,  0.2744, -0.8795,  ...,  0.1696, -0.2222,  1.9124]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.2442,  0.3822, -0.5899,  ..., -0.5602, -2.4538,  0.5828]],\n",
      "\n",
      "         [[-0.0827,  0.0116,  0.2914,  ..., -0.6552, -0.1209,  0.1349]],\n",
      "\n",
      "         [[-1.3761,  0.3145, -0.3789,  ...,  0.5463, -0.0247, -0.6759]]]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[[[-0.0860,  0.0723,  0.0119,  ...,  0.0566, -0.0074, -0.0821]],\n",
      "\n",
      "         [[-0.0049,  0.0067,  0.0484,  ..., -0.0373,  0.0270, -0.0130]],\n",
      "\n",
      "         [[ 0.0043, -0.1216,  0.0674,  ..., -0.0080, -0.0173, -0.0125]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0640,  0.1282,  0.0360,  ..., -0.0113, -0.1061,  0.0397]],\n",
      "\n",
      "         [[ 0.0787, -0.1928,  0.1280,  ..., -0.0240,  0.2259,  0.0362]],\n",
      "\n",
      "         [[ 0.0011, -0.0250, -0.0602,  ...,  0.0657,  0.0108, -0.0717]]]],\n",
      "       device='cuda:0', requires_grad=True)), (Parameter containing:\n",
      "tensor([[[[ 2.4887e-01, -3.3334e-02,  3.9967e-02,  ..., -2.9484e-01,\n",
      "           -9.2976e-02,  6.4345e-02]],\n",
      "\n",
      "         [[-5.6617e-02,  4.0968e-02, -2.6456e-01,  ...,  6.9859e-01,\n",
      "            3.6781e-02, -1.3626e-01]],\n",
      "\n",
      "         [[ 4.3353e-01, -2.8450e-01, -5.7408e-02,  ...,  6.0191e-01,\n",
      "            2.5968e-01, -3.6691e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.4719e-04,  7.8164e-02, -6.5022e-02,  ...,  1.4657e-01,\n",
      "           -1.0152e-01, -3.0135e-03]],\n",
      "\n",
      "         [[ 1.9649e-01,  2.0610e+00,  2.6964e+00,  ..., -2.4135e-02,\n",
      "           -5.4627e-01, -3.1368e-02]],\n",
      "\n",
      "         [[-2.2185e-01, -7.0477e-01, -2.1211e-01,  ..., -9.7633e-01,\n",
      "           -3.1630e-02, -9.4551e-01]]]], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[[[-2.3173e-02,  3.7630e-02, -1.7019e-02,  ..., -9.8371e-02,\n",
      "           -2.7196e-04,  1.2524e-02]],\n",
      "\n",
      "         [[-4.3468e-02,  7.9814e-03, -2.8150e-03,  ..., -1.8606e-01,\n",
      "            1.3309e-02, -3.4531e-02]],\n",
      "\n",
      "         [[-9.1365e-03, -1.9652e-02, -1.2628e-01,  ..., -3.0481e-02,\n",
      "            2.7589e-02, -8.8173e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.5148e-02, -2.1038e-02,  1.2730e-02,  ..., -1.4672e-02,\n",
      "            3.9222e-02,  1.9870e-02]],\n",
      "\n",
      "         [[-3.1179e-02,  7.0826e-05, -9.7464e-02,  ...,  7.7960e-02,\n",
      "           -1.4064e-02, -1.2819e-01]],\n",
      "\n",
      "         [[ 2.6853e-02, -2.6784e-02,  1.0370e-02,  ...,  2.8811e-02,\n",
      "            5.0502e-02,  3.4383e-02]]]], device='cuda:0', requires_grad=True)), (Parameter containing:\n",
      "tensor([[[[ 0.1079, -0.1666,  0.1543,  ...,  0.0166,  0.0047, -0.1078]],\n",
      "\n",
      "         [[-0.0390,  0.2035,  0.3727,  ..., -0.1532, -0.0971,  0.1342]],\n",
      "\n",
      "         [[-0.1173,  0.5948, -0.4503,  ..., -0.0366, -0.0361,  0.0138]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.3627,  0.3491,  0.3740,  ...,  0.2332, -0.3305, -0.1514]],\n",
      "\n",
      "         [[-1.4521,  0.1458, -0.0239,  ...,  0.2169,  0.2847, -0.5234]],\n",
      "\n",
      "         [[-0.0124,  0.2807, -0.1024,  ...,  0.5600,  0.2632,  0.2271]]]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[[[-2.8875e-02,  9.5762e-03,  3.0611e-02,  ...,  2.9470e-02,\n",
      "           -3.9193e-02,  2.5109e-02]],\n",
      "\n",
      "         [[-1.8521e-01, -2.4575e-02,  9.9089e-02,  ..., -6.3981e-02,\n",
      "            2.3308e-02, -9.2164e-02]],\n",
      "\n",
      "         [[ 9.1153e-01,  4.7899e-03, -1.2278e-02,  ...,  2.0345e+00,\n",
      "           -4.5605e-02,  9.4107e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-4.1235e-03,  3.9049e-02, -5.3541e-02,  ...,  6.9787e-02,\n",
      "           -2.9254e-02, -2.8341e-02]],\n",
      "\n",
      "         [[ 1.2214e-01, -4.4916e-02, -6.0088e-02,  ..., -4.4774e-02,\n",
      "            3.8636e-02,  6.3778e-02]],\n",
      "\n",
      "         [[ 3.6004e-02, -3.1649e-02,  6.9217e-03,  ...,  1.1888e-03,\n",
      "           -1.7592e-02, -8.0654e-04]]]], device='cuda:0', requires_grad=True)), (Parameter containing:\n",
      "tensor([[[[ 0.0544,  2.2324, -0.4207,  ..., -0.7434, -0.1308,  0.0676]],\n",
      "\n",
      "         [[-0.2034,  1.8987, -0.6391,  ...,  0.0853,  0.0665,  0.0266]],\n",
      "\n",
      "         [[ 0.5735, -0.5035, -0.3687,  ...,  0.8958,  0.9987,  0.8873]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1712, -0.3043,  0.0258,  ...,  0.0434,  0.6704, -0.5878]],\n",
      "\n",
      "         [[ 0.3819, -0.2638, -0.1407,  ...,  0.0281, -0.3387, -2.2901]],\n",
      "\n",
      "         [[-0.6775,  0.1819, -0.5750,  ...,  1.5186,  1.1686, -1.0203]]]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[[[ 0.0081,  0.0937,  0.0003,  ..., -0.0344,  0.0735, -0.0226]],\n",
      "\n",
      "         [[ 0.0018, -0.0508, -0.1430,  ..., -0.0438,  0.0251, -0.0137]],\n",
      "\n",
      "         [[ 0.1025, -0.0249, -0.0418,  ..., -0.0033, -0.0787,  0.0412]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0005,  0.0171,  0.0239,  ...,  0.0443, -0.0336, -0.0239]],\n",
      "\n",
      "         [[-0.1281, -0.0785, -0.0278,  ...,  0.1091, -0.0485,  0.1036]],\n",
      "\n",
      "         [[ 0.0529, -0.0060,  0.0187,  ..., -0.0354, -0.0093,  0.0550]]]],\n",
      "       device='cuda:0', requires_grad=True)), (Parameter containing:\n",
      "tensor([[[[ 1.4474, -0.1850, -1.2375,  ...,  0.1358, -1.0232, -0.3125]],\n",
      "\n",
      "         [[-0.4307,  0.1484,  0.0885,  ..., -0.3127,  0.4366, -0.0793]],\n",
      "\n",
      "         [[-0.1452,  0.3206, -0.7232,  ..., -0.4819, -0.7885, -0.0022]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.9598,  2.0390,  0.1845,  ...,  0.0480,  0.0237, -0.0210]],\n",
      "\n",
      "         [[ 0.5546, -0.0413, -0.0545,  ...,  0.9445,  0.1364,  0.2789]],\n",
      "\n",
      "         [[-0.2342,  0.3007, -0.3174,  ..., -0.3280, -0.0871,  0.2516]]]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[[[-0.0895, -0.0509,  0.0092,  ...,  0.0675, -0.0005,  0.0130]],\n",
      "\n",
      "         [[ 0.0054,  0.0284,  0.0095,  ...,  0.0374,  0.0214,  0.0090]],\n",
      "\n",
      "         [[-0.0413, -0.0278, -0.0021,  ...,  0.0898, -0.0983,  0.0625]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0018,  0.0393, -0.0942,  ...,  0.0121,  0.2016,  0.0756]],\n",
      "\n",
      "         [[-0.0354,  0.0019, -0.3315,  ...,  0.0126,  0.0016, -0.0258]],\n",
      "\n",
      "         [[ 0.0112,  0.0042, -0.0532,  ...,  0.0512,  0.0433, -0.2011]]]],\n",
      "       device='cuda:0', requires_grad=True)), (Parameter containing:\n",
      "tensor([[[[ 3.3872e-01, -4.7841e-01, -4.9322e-01,  ..., -1.1528e-01,\n",
      "            4.7944e-01,  3.1496e-01]],\n",
      "\n",
      "         [[-3.2830e-01,  1.0486e-01, -7.5507e-01,  ...,  7.3543e-01,\n",
      "           -1.8489e+00, -6.4158e-01]],\n",
      "\n",
      "         [[-1.5006e+00, -2.0448e+00, -5.3921e-01,  ..., -2.0328e+00,\n",
      "            9.7479e-01, -2.2526e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.2686e+00, -1.5607e+00, -7.6203e-01,  ..., -6.9340e-01,\n",
      "           -9.5980e-02,  1.5894e-02]],\n",
      "\n",
      "         [[-3.2884e-01, -6.2781e-01,  5.4881e-02,  ..., -5.4287e-01,\n",
      "            5.5332e-01, -1.1012e+00]],\n",
      "\n",
      "         [[ 8.1505e-02, -1.8291e-03, -7.6225e-01,  ...,  6.5645e-01,\n",
      "            3.4326e-01,  2.1487e+00]]]], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[[[-0.0167, -0.1137, -0.0291,  ..., -0.0181, -0.0264,  0.0244]],\n",
      "\n",
      "         [[-0.0329, -0.0652,  0.0208,  ..., -0.0473,  0.0102,  0.0027]],\n",
      "\n",
      "         [[ 0.0015,  0.0485,  0.0159,  ...,  0.0054, -0.0312, -0.0404]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1648,  0.1316,  0.1078,  ...,  0.0785, -0.1302, -0.0358]],\n",
      "\n",
      "         [[ 0.0253, -0.0850,  0.0455,  ..., -0.0970,  0.0254, -0.0393]],\n",
      "\n",
      "         [[ 0.1060,  0.0229, -0.0444,  ..., -0.0450, -0.1361, -0.0554]]]],\n",
      "       device='cuda:0', requires_grad=True)), (Parameter containing:\n",
      "tensor([[[[-0.4711, -1.1854,  0.7055,  ..., -0.0542, -0.9185,  0.2022]],\n",
      "\n",
      "         [[ 1.1467,  0.1977, -0.2666,  ..., -1.8705, -2.2500,  2.4069]],\n",
      "\n",
      "         [[ 1.8530, -0.3008, -1.2798,  ..., -1.1909, -1.1210,  0.3025]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5294, -0.3239,  0.3758,  ..., -0.7061,  0.6882,  0.7135]],\n",
      "\n",
      "         [[ 1.3270,  2.3736, -0.9790,  ..., -0.3838, -0.3718, -0.9716]],\n",
      "\n",
      "         [[-0.4986,  0.3020,  0.5080,  ..., -0.4074,  0.7464, -0.4372]]]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[[[ 0.0347, -0.0320, -0.0796,  ...,  0.0412,  0.0428, -0.0319]],\n",
      "\n",
      "         [[-0.0285,  0.0085, -0.0186,  ...,  0.0050,  0.0260, -0.0556]],\n",
      "\n",
      "         [[-0.0301,  0.0145, -0.0617,  ...,  0.0245, -0.0203,  0.0298]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0500, -0.0313,  0.0439,  ...,  0.0111,  0.1262,  0.0931]],\n",
      "\n",
      "         [[ 0.0977, -0.0852,  0.0359,  ...,  0.0703,  0.0186,  0.0031]],\n",
      "\n",
      "         [[ 0.0559, -0.1200,  0.1503,  ..., -0.0855,  0.0438,  0.0072]]]],\n",
      "       device='cuda:0', requires_grad=True)), (Parameter containing:\n",
      "tensor([[[[-1.4122, -0.3196,  1.6215,  ...,  0.1801,  2.0927, -1.4780]],\n",
      "\n",
      "         [[-1.9722,  0.1924, -0.0499,  ..., -0.7922, -0.8024,  0.2284]],\n",
      "\n",
      "         [[-0.1291, -0.4358, -0.7298,  ..., -1.2640,  0.6273, -0.7337]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.3364, -0.9558,  0.1783,  ...,  2.0962, -0.2074,  0.7414]],\n",
      "\n",
      "         [[-0.7342,  1.6328, -0.4655,  ...,  0.0648,  1.4248,  0.9460]],\n",
      "\n",
      "         [[ 1.0609, -1.4040,  0.6930,  ...,  0.7296,  2.4996, -0.8988]]]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[[[ 0.0369, -0.0281, -0.0194,  ...,  0.0392,  0.0352, -0.0059]],\n",
      "\n",
      "         [[ 0.0097,  0.0038, -0.0250,  ..., -0.0089, -0.0355,  0.0042]],\n",
      "\n",
      "         [[-0.0002,  0.0317,  0.0219,  ..., -0.0045,  0.0155,  0.0117]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0024,  0.0155,  0.0375,  ..., -0.0239, -0.0012,  0.0211]],\n",
      "\n",
      "         [[ 0.0489,  0.0152, -0.0099,  ..., -0.0281,  0.0789,  0.0798]],\n",
      "\n",
      "         [[ 0.0096,  0.0530,  0.0575,  ...,  0.0464, -0.0743, -0.0647]]]],\n",
      "       device='cuda:0', requires_grad=True)), (Parameter containing:\n",
      "tensor([[[[ 0.9442, -0.1537,  0.7208,  ...,  1.0253, -0.6385,  0.0544]],\n",
      "\n",
      "         [[ 0.4494,  1.0048, -3.2212,  ..., -1.3578,  0.6664,  0.5700]],\n",
      "\n",
      "         [[-1.9129,  2.4953,  0.6243,  ..., -0.9344, -0.2284,  0.4562]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.3143,  0.5261, -1.4482,  ..., -0.2381,  0.8956,  1.1253]],\n",
      "\n",
      "         [[ 0.2620,  1.2411, -0.5484,  ...,  0.1728,  0.1158, -0.1997]],\n",
      "\n",
      "         [[-0.0448, -0.2309,  0.0998,  ..., -0.2956, -0.0776, -0.1022]]]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[[[ 0.0199,  0.0248,  0.1030,  ...,  0.0062,  0.0046,  0.0332]],\n",
      "\n",
      "         [[-0.0382, -0.0058, -0.0432,  ..., -0.0359,  0.0754,  0.0063]],\n",
      "\n",
      "         [[-0.0035,  0.0058, -0.0514,  ...,  0.0540, -0.0128,  0.0186]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0284,  0.0312, -0.0476,  ...,  0.0440, -0.0156,  0.0424]],\n",
      "\n",
      "         [[-0.0236, -0.0234, -0.0249,  ...,  0.0250, -0.0657,  0.0104]],\n",
      "\n",
      "         [[-0.0236,  0.0325, -0.0510,  ..., -0.0333, -0.0065,  0.0202]]]],\n",
      "       device='cuda:0', requires_grad=True)), (Parameter containing:\n",
      "tensor([[[[ 0.2096, -0.5556, -0.0396,  ...,  0.1866, -0.7614, -0.3287]],\n",
      "\n",
      "         [[-0.4913,  0.1837, -0.5997,  ..., -0.0739,  0.4378,  2.0829]],\n",
      "\n",
      "         [[ 0.4983, -1.1787,  1.0895,  ...,  0.6594, -0.1787,  0.0495]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.7536,  1.9582, -1.5931,  ...,  0.5070,  1.6169, -2.7838]],\n",
      "\n",
      "         [[ 0.7888,  0.9570,  1.7188,  ...,  1.8974, -0.5746,  1.0141]],\n",
      "\n",
      "         [[-0.5631, -0.3047, -0.1162,  ..., -0.2106,  0.4260,  0.1495]]]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[[[-0.0271,  0.0203,  0.0504,  ...,  0.0520,  0.0830,  0.0254]],\n",
      "\n",
      "         [[-0.0301, -0.1338, -0.0558,  ...,  0.0841, -0.0615, -0.1190]],\n",
      "\n",
      "         [[-0.0092, -0.0341, -0.0260,  ..., -0.0291,  0.0024, -0.1231]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0458, -0.0359,  0.0229,  ..., -0.0165, -0.0039, -0.0411]],\n",
      "\n",
      "         [[ 0.0253,  0.0354,  0.0608,  ..., -0.0102,  0.0979,  0.0338]],\n",
      "\n",
      "         [[-0.0611, -0.0194,  0.0418,  ...,  0.0392, -0.0002, -0.0155]]]],\n",
      "       device='cuda:0', requires_grad=True)), (Parameter containing:\n",
      "tensor([[[[ 0.9358,  2.0111, -0.1201,  ...,  0.5293, -0.5361, -0.7653]],\n",
      "\n",
      "         [[-0.0151,  0.0294, -0.0982,  ..., -0.4073,  0.8229,  0.1665]],\n",
      "\n",
      "         [[-0.2572,  0.3560,  0.5354,  ...,  0.9433,  0.2667, -3.3165]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.6925, -0.2815, -1.6162,  ..., -0.2991,  0.1091,  1.3398]],\n",
      "\n",
      "         [[-0.4239, -1.3689, -1.1214,  ..., -1.3930, -0.7294,  2.1003]],\n",
      "\n",
      "         [[-0.7987, -0.0940, -0.3879,  ...,  2.2149,  2.6081,  0.1799]]]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[[[ 0.0350, -0.0160,  0.0371,  ...,  0.0349,  0.0188, -0.0040]],\n",
      "\n",
      "         [[ 0.1487,  0.0865,  0.0393,  ..., -0.1342, -0.0480,  0.0769]],\n",
      "\n",
      "         [[-0.0388, -0.0010, -0.0195,  ...,  0.0003,  0.0107,  0.0256]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0334,  0.0350, -0.0310,  ..., -0.2119, -0.0303,  0.0635]],\n",
      "\n",
      "         [[-0.0488, -0.1482, -0.0165,  ..., -0.0370, -0.0252,  0.1177]],\n",
      "\n",
      "         [[-0.0231, -0.0376, -0.1029,  ...,  0.0167, -0.0691,  0.0316]]]],\n",
      "       device='cuda:0', requires_grad=True)), (Parameter containing:\n",
      "tensor([[[[ 0.2637,  0.6346, -2.1219,  ...,  0.4776,  0.3400, -0.4099]],\n",
      "\n",
      "         [[-0.2878,  0.0295, -0.0094,  ...,  0.2580, -0.0566,  0.1506]],\n",
      "\n",
      "         [[-0.0994,  1.0671,  0.4918,  ..., -0.3453,  1.2715, -0.7508]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0760,  0.2535, -0.5532,  ...,  1.0390,  1.6518, -0.6113]],\n",
      "\n",
      "         [[-1.1074, -0.0559, -0.1189,  ..., -0.2845, -0.1953, -0.9208]],\n",
      "\n",
      "         [[-0.6261, -1.7170,  0.7449,  ...,  1.4990,  0.9224,  0.7517]]]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[[[ 0.0128,  0.0435, -0.0003,  ...,  0.0441, -0.0265, -0.0487]],\n",
      "\n",
      "         [[ 0.0143, -0.0130, -0.0092,  ..., -0.0029,  0.0210,  0.0561]],\n",
      "\n",
      "         [[-0.0661, -0.0731,  0.0466,  ...,  0.0263,  0.0222,  0.0001]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0018, -0.0051,  0.0225,  ..., -0.0031, -0.0374, -0.0003]],\n",
      "\n",
      "         [[-0.0139, -0.0311, -0.0922,  ..., -0.0509, -0.0202, -0.0078]],\n",
      "\n",
      "         [[ 0.0173, -0.0172,  0.0016,  ..., -0.0024,  0.0155, -0.0454]]]],\n",
      "       device='cuda:0', requires_grad=True)), (Parameter containing:\n",
      "tensor([[[[-0.0286,  0.1253,  0.0507,  ..., -0.0263,  0.0472, -0.0333]],\n",
      "\n",
      "         [[ 0.0220,  0.3506,  0.2449,  ..., -0.5223, -0.0755,  1.4895]],\n",
      "\n",
      "         [[ 0.2722, -0.6975, -1.1560,  ...,  0.0403, -0.5172, -0.9596]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.5547,  0.4143, -1.9651,  ..., -0.2534, -1.1875,  0.6351]],\n",
      "\n",
      "         [[ 0.7921,  0.1030,  0.3561,  ..., -0.4645, -0.1168,  1.3611]],\n",
      "\n",
      "         [[-1.3627, -1.6212, -0.0286,  ...,  1.6311,  0.8740,  0.4341]]]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[[[-0.0338, -0.0883, -0.0157,  ..., -0.0763,  0.1136,  0.1034]],\n",
      "\n",
      "         [[-0.0461,  0.0256, -0.0427,  ...,  0.0125,  0.0477,  0.0555]],\n",
      "\n",
      "         [[ 0.0300,  0.0297, -0.0289,  ..., -0.0225, -0.0252,  0.0096]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0309, -0.0147, -0.0037,  ...,  0.0171, -0.0696,  0.0650]],\n",
      "\n",
      "         [[ 0.0202, -0.0034, -0.0137,  ..., -0.0294,  0.0184, -0.0300]],\n",
      "\n",
      "         [[ 0.0324,  0.0048, -0.0217,  ..., -0.0284,  0.0283, -0.0009]]]],\n",
      "       device='cuda:0', requires_grad=True)), (Parameter containing:\n",
      "tensor([[[[ 0.3140,  0.4563,  0.7979,  ...,  0.6694,  0.4385, -1.0718]],\n",
      "\n",
      "         [[-0.1807, -0.3689, -0.1305,  ...,  0.5213, -0.6126, -0.2470]],\n",
      "\n",
      "         [[ 0.1456, -0.3343,  0.2459,  ...,  0.3701, -0.3221,  0.3043]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.2799, -0.7368, -0.4858,  ...,  0.5508,  0.8189,  0.2265]],\n",
      "\n",
      "         [[ 0.1646, -0.2023,  0.1135,  ..., -0.1858,  0.1015, -0.6303]],\n",
      "\n",
      "         [[-0.9878,  0.6999, -0.5509,  ...,  1.1799,  0.6474,  1.0660]]]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[[[-1.9698e-02,  2.6103e-02, -1.0468e-01,  ...,  4.0017e-02,\n",
      "           -4.8882e-02,  4.1670e-02]],\n",
      "\n",
      "         [[-3.7891e-02,  4.3253e-02,  9.8856e-03,  ..., -1.8925e-02,\n",
      "           -1.6653e-02,  3.3328e-02]],\n",
      "\n",
      "         [[ 2.1206e-02, -5.4721e-02,  5.1184e-02,  ...,  2.5540e-02,\n",
      "            5.8894e-02, -4.8994e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-5.6469e-02,  2.0382e-02,  1.9793e-02,  ...,  1.5923e-02,\n",
      "           -7.2780e-02, -8.1682e-04]],\n",
      "\n",
      "         [[-5.2653e-05,  2.7654e-02,  7.8645e-03,  ...,  5.8324e-03,\n",
      "           -4.8779e-02, -5.9417e-02]],\n",
      "\n",
      "         [[ 6.1574e-02,  2.1647e-02, -1.0587e-02,  ..., -5.6930e-02,\n",
      "            6.3181e-02,  2.6344e-02]]]], device='cuda:0', requires_grad=True)), (Parameter containing:\n",
      "tensor([[[[-0.5080,  0.5405, -0.0557,  ...,  0.3689,  0.8104, -0.9438]],\n",
      "\n",
      "         [[ 0.1657, -2.0390,  0.0695,  ..., -0.0530, -0.2435,  0.0941]],\n",
      "\n",
      "         [[ 0.2925,  0.3302, -0.0321,  ...,  0.4641,  0.5389,  0.1984]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9699,  0.2404,  0.2865,  ..., -0.6544,  0.3161,  0.4660]],\n",
      "\n",
      "         [[-0.0351,  0.2615,  0.0309,  ...,  0.2451, -0.5482, -0.1743]],\n",
      "\n",
      "         [[-0.3736, -1.6021,  0.0050,  ..., -0.5642,  0.3721, -0.7304]]]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[[[-0.0083,  0.0004, -0.0739,  ...,  0.0279, -0.0096,  0.0312]],\n",
      "\n",
      "         [[ 0.0192,  0.0221, -0.0449,  ..., -0.0121,  0.0329,  0.0164]],\n",
      "\n",
      "         [[-0.0217, -0.1340, -0.0467,  ...,  0.0240,  0.0340,  0.0703]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0308,  0.0980, -0.0822,  ..., -0.0404, -0.1085, -0.1203]],\n",
      "\n",
      "         [[ 0.0562,  0.1730, -0.0499,  ..., -0.0577,  0.0195,  0.0563]],\n",
      "\n",
      "         [[-0.0037, -0.0276,  0.0127,  ...,  0.0163, -0.0549, -0.0722]]]],\n",
      "       device='cuda:0', requires_grad=True)), (Parameter containing:\n",
      "tensor([[[[-2.2169e-01, -1.3227e-01,  3.9294e-01,  ...,  2.8870e-01,\n",
      "           -3.4068e-01, -2.7248e-01]],\n",
      "\n",
      "         [[-2.8883e-01, -4.6277e-03,  6.3961e-02,  ..., -2.6854e-02,\n",
      "           -5.9172e-02, -1.0467e+00]],\n",
      "\n",
      "         [[-7.0617e-01,  2.1149e+00,  5.3334e-01,  ..., -2.2202e-01,\n",
      "            1.1113e-01,  2.1728e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.5501e-03, -4.0959e-01,  8.7484e-02,  ..., -4.4179e-01,\n",
      "            3.6512e-01,  2.3398e-01]],\n",
      "\n",
      "         [[-3.3416e-03, -7.4272e-01, -7.2583e-02,  ...,  2.5842e-01,\n",
      "            1.2988e-01,  1.8572e-01]],\n",
      "\n",
      "         [[ 5.1797e-02,  1.9767e-01, -1.1141e-01,  ...,  6.8376e-03,\n",
      "           -1.9400e-01,  8.5948e-01]]]], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[[[ 0.0519, -0.0288,  0.0424,  ..., -0.0397,  0.1047,  0.0296]],\n",
      "\n",
      "         [[-0.1401,  0.1136, -0.1777,  ...,  0.1614, -0.0620,  0.1351]],\n",
      "\n",
      "         [[ 0.0776,  0.0356, -0.0294,  ...,  0.0231, -0.0018,  0.0135]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0134,  0.0674, -0.0301,  ...,  0.0441,  0.0043, -0.0589]],\n",
      "\n",
      "         [[-0.0355, -0.0083, -0.0389,  ..., -0.0490,  0.0436,  0.0024]],\n",
      "\n",
      "         [[ 0.0755, -0.0048, -0.0571,  ...,  0.0090,  0.0595,  0.0149]]]],\n",
      "       device='cuda:0', requires_grad=True)), (Parameter containing:\n",
      "tensor([[[[-0.2380, -0.1549,  2.1220,  ...,  0.2077,  0.4133, -0.2428]],\n",
      "\n",
      "         [[ 1.3268,  0.9297,  1.5445,  ...,  1.2806, -1.3515, -0.8024]],\n",
      "\n",
      "         [[ 0.2170,  0.1851, -0.2107,  ...,  0.0254, -0.2555, -0.0588]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0412, -2.1047, -0.0084,  ..., -0.2667,  0.5192,  0.3638]],\n",
      "\n",
      "         [[-0.0452,  0.1424,  0.7154,  ...,  1.3202,  0.0082, -0.2703]],\n",
      "\n",
      "         [[-0.3554,  0.5591, -0.6960,  ..., -0.1333,  0.4743, -0.7326]]]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[[[ 0.0126,  0.0824, -0.0968,  ...,  0.1584,  0.0222, -0.0550]],\n",
      "\n",
      "         [[ 0.0042,  0.0563,  0.0808,  ...,  0.0316,  0.0226, -0.0386]],\n",
      "\n",
      "         [[ 0.0071, -0.0558,  0.0028,  ..., -0.0831,  0.0650, -0.0089]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0596, -0.0455, -0.0922,  ..., -0.0478, -0.0069, -0.0416]],\n",
      "\n",
      "         [[-0.0495, -0.0041,  0.0170,  ..., -0.0080,  0.0495,  0.0436]],\n",
      "\n",
      "         [[ 0.0242, -0.0208, -0.0136,  ...,  0.0155, -0.0556,  0.0643]]]],\n",
      "       device='cuda:0', requires_grad=True)), (Parameter containing:\n",
      "tensor([[[[ 0.0354, -0.1054,  0.1649,  ...,  2.2574,  0.1236,  0.2863]],\n",
      "\n",
      "         [[-0.0032, -0.0850,  2.2256,  ...,  0.0824, -0.3385, -0.4023]],\n",
      "\n",
      "         [[-0.2564, -0.3527,  0.3724,  ...,  0.0923,  0.4598,  0.3224]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1754,  0.4710,  0.6686,  ...,  0.1753, -0.3196,  0.1982]],\n",
      "\n",
      "         [[-0.6675,  0.2980,  0.0523,  ..., -0.8828, -0.3547, -0.0423]],\n",
      "\n",
      "         [[-0.1486, -0.5200,  0.1192,  ...,  0.3898,  0.0403, -0.3126]]]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[[[-0.0390,  0.0836, -0.0112,  ..., -0.0225, -0.0101, -0.0128]],\n",
      "\n",
      "         [[-0.1801, -0.0326,  0.1084,  ..., -0.0452, -0.0146, -0.0075]],\n",
      "\n",
      "         [[ 0.0843,  0.0385,  0.2359,  ...,  0.0351, -0.1466, -0.0263]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0511, -0.1791, -0.0102,  ..., -0.0226, -0.1763,  0.0778]],\n",
      "\n",
      "         [[-0.0120,  0.5080,  0.2550,  ...,  0.0335,  0.0083,  0.0520]],\n",
      "\n",
      "         [[ 0.0210,  0.0786, -0.0401,  ..., -0.0165, -0.5679,  0.1094]]]],\n",
      "       device='cuda:0', requires_grad=True)), (Parameter containing:\n",
      "tensor([[[[ 0.1998,  0.5588, -0.0127,  ...,  0.9735,  0.1872, -1.0818]],\n",
      "\n",
      "         [[-0.4288, -0.4970, -0.3433,  ...,  0.1074, -0.0307,  0.2859]],\n",
      "\n",
      "         [[ 0.6357,  0.3820,  2.8418,  ...,  0.1326, -0.8608, -0.3890]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.2227, -0.3414,  0.5459,  ...,  0.5440,  0.0175,  0.1044]],\n",
      "\n",
      "         [[-0.1125, -0.1415, -0.1319,  ..., -0.5427, -0.3252, -0.2656]],\n",
      "\n",
      "         [[-0.5211,  0.2885,  0.3566,  ...,  0.4352,  0.1388, -0.0897]]]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[[[ 0.1029,  0.0240, -0.0505,  ...,  0.0303, -0.0281, -0.0991]],\n",
      "\n",
      "         [[-0.0480,  0.2427,  0.1493,  ..., -0.0161, -0.0819, -0.0540]],\n",
      "\n",
      "         [[ 0.5430,  0.5643, -0.1245,  ..., -0.2880, -0.0494,  0.1173]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0091,  0.0031,  0.2056,  ...,  0.0045,  0.1870, -0.0225]],\n",
      "\n",
      "         [[ 0.0851,  0.1370, -0.0144,  ..., -0.0037, -0.1339,  0.0355]],\n",
      "\n",
      "         [[-0.3384, -0.2847, -0.2184,  ...,  0.0450,  0.1296,  0.3437]]]],\n",
      "       device='cuda:0', requires_grad=True)))\n",
      "\n",
      "\n",
      "\n",
      "=====Iteration: 2=====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context + prompt\n",
      "<|endoftext|>My favorite music genre is death metal. The genre was born out of the need for something that would feel right in your stomach and not feel too weird, yet not feel too weird for you to listen to while eating.\n",
      "<|endoftext|>I listen to rap music. I don't listen to music as an artform, but to music as a medium.\n",
      "<|endoftext|>I workout four hours a day. I am an avid weightlifter.\n",
      "<|endoftext|>I'm a christian. I believe in Jesus and love Jesus and want him to be my king, my prophet, our savior.\n",
      "************************************************************\n",
      "response: \n",
      " Death metal is about being dead.\n",
      " I listen to rap music as a medium, because rap music is a unique form of music that has become popular in recent years.\n",
      " My goal is to achieve and maintain the same weight I had before I started my diet.\n",
      " But I know that he is not my Messiah; I know he doesn't even have a name.\n",
      "************************************************************\n",
      "discrim loss: 2.053444\n",
      "\n",
      "=======update loss: 2.053444=======\n",
      "((Parameter containing:\n",
      "tensor([[[[ 0.3492,  0.1139,  0.3395,  ..., -0.6057,  0.1059, -0.1620]],\n",
      "\n",
      "         [[-0.4678, -0.8882, -0.9204,  ...,  0.1143,  0.4527,  0.5019]],\n",
      "\n",
      "         [[-0.4594, -0.5928, -0.3890,  ...,  0.5879, -0.0475, -0.0135]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.3854, -0.3938, -0.0681,  ...,  0.2084,  0.4516, -0.5337]],\n",
      "\n",
      "         [[ 0.7943, -0.8372, -0.3881,  ...,  0.0129, -1.1219,  0.4492]],\n",
      "\n",
      "         [[ 0.3224,  0.4198, -0.3096,  ...,  0.6979, -1.6785, -1.0112]]]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[[[ 0.0296, -0.0327, -0.0147,  ..., -0.0245,  0.0347,  0.0272]],\n",
      "\n",
      "         [[ 0.0064,  0.0144,  0.0069,  ...,  0.0489,  0.0085,  0.0076]],\n",
      "\n",
      "         [[ 0.0053,  0.0317, -0.0143,  ...,  0.0320, -0.0324, -0.0580]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0272, -0.0731,  0.0179,  ...,  0.0689,  0.0301, -0.0146]],\n",
      "\n",
      "         [[-0.0136, -0.0111, -0.0048,  ...,  0.0195,  0.0179, -0.0024]],\n",
      "\n",
      "         [[ 0.0692, -0.0228,  0.0154,  ...,  0.0317, -0.0350, -0.0005]]]],\n",
      "       device='cuda:0', requires_grad=True)), (Parameter containing:\n",
      "tensor([[[[ 0.1391,  1.0255, -0.2042,  ...,  0.6696, -2.0426,  0.1553]],\n",
      "\n",
      "         [[-1.8233, -0.1778,  3.2326,  ..., -0.2835,  0.4411, -0.2435]],\n",
      "\n",
      "         [[ 1.4732,  1.3997,  1.6481,  ...,  2.1500,  0.2041, -1.0800]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.7688, -0.3113,  0.6768,  ...,  0.1361, -0.4873,  0.6767]],\n",
      "\n",
      "         [[-0.5062,  0.3318, -0.2477,  ...,  0.1098, -0.0539, -0.6015]],\n",
      "\n",
      "         [[ 0.0406, -0.5826, -0.4525,  ...,  0.6559,  3.9276,  0.2619]]]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[[[ 6.1878e-02,  5.0388e-02,  3.9478e-01,  ..., -1.9864e-02,\n",
      "            1.9692e-01, -1.4137e-01]],\n",
      "\n",
      "         [[ 2.7948e-01, -6.9346e-01, -9.2666e-02,  ...,  1.7241e-01,\n",
      "            3.2254e-01,  4.1350e-02]],\n",
      "\n",
      "         [[-3.6594e-01,  2.0117e-01, -1.4194e-01,  ..., -6.3705e-02,\n",
      "            1.7212e-01,  1.9592e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.1965e-01,  2.0814e-01,  1.2948e-01,  ..., -4.1878e-01,\n",
      "           -3.3030e-01, -7.1638e-01]],\n",
      "\n",
      "         [[ 1.5744e-01,  2.3902e-01, -2.9023e-01,  ...,  2.1899e-01,\n",
      "            1.7138e-01,  3.7095e-04]],\n",
      "\n",
      "         [[-6.7463e-02, -4.5148e-02,  3.3464e-02,  ..., -4.2214e-01,\n",
      "            4.2968e-01, -1.3626e-01]]]], device='cuda:0', requires_grad=True)), (Parameter containing:\n",
      "tensor([[[[ 0.1665, -0.3277, -0.1477,  ..., -0.1414,  0.1293, -0.0685]],\n",
      "\n",
      "         [[ 0.2934, -0.2481, -0.0247,  ..., -0.8269, -2.1581, -0.0348]],\n",
      "\n",
      "         [[ 0.1203,  0.9742, -0.4217,  ..., -0.7878, -0.5152, -0.2153]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.8572, -0.1355, -0.2883,  ...,  0.6588,  0.0088,  0.6194]],\n",
      "\n",
      "         [[ 0.9781, -1.3044, -1.1334,  ...,  0.7437,  0.0807, -1.5605]],\n",
      "\n",
      "         [[ 1.6633,  0.6861,  0.7867,  ..., -0.0886, -0.1465, -0.2833]]]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[[[ 0.2841, -0.4201,  0.2447,  ...,  0.7521,  0.0756,  0.2141]],\n",
      "\n",
      "         [[-0.1222,  0.2392,  0.1736,  ..., -0.0545,  0.0679,  0.0718]],\n",
      "\n",
      "         [[ 0.1256, -1.4394,  0.2765,  ..., -0.3724,  0.4095,  0.4913]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.2121, -0.0380, -0.0566,  ..., -0.0175,  0.1606, -0.0659]],\n",
      "\n",
      "         [[ 0.3528,  0.3991,  1.0272,  ...,  0.1197,  0.0519,  0.1442]],\n",
      "\n",
      "         [[-0.3327, -0.0931, -0.2200,  ...,  0.0763,  0.0086, -0.1525]]]],\n",
      "       device='cuda:0', requires_grad=True)), (Parameter containing:\n",
      "tensor([[[[-0.5805, -1.1831,  0.1795,  ...,  0.9751, -0.1935, -0.1294]],\n",
      "\n",
      "         [[-0.3130, -0.0894, -0.4984,  ..., -0.4103,  0.2008, -1.5121]],\n",
      "\n",
      "         [[-0.1154, -0.0397,  0.2145,  ..., -0.1883,  0.4491,  0.4996]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.3107, -0.8724, -0.0238,  ..., -1.0777,  1.0735,  0.8316]],\n",
      "\n",
      "         [[ 0.6101,  0.5532, -0.5339,  ...,  0.2068,  0.4547,  0.7082]],\n",
      "\n",
      "         [[ 0.8895, -1.0163, -0.0726,  ..., -1.0464, -0.0098,  0.8951]]]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[[[-3.5288e-02,  2.3370e-02,  5.6026e-02,  ..., -3.3260e-01,\n",
      "            1.3462e-01, -7.9660e-03]],\n",
      "\n",
      "         [[-3.1801e-02,  2.0379e-02,  7.1083e-02,  ...,  8.5985e-02,\n",
      "           -3.9861e-02,  6.7278e-02]],\n",
      "\n",
      "         [[-2.0311e-02, -5.5232e-01,  4.2300e-02,  ..., -7.7056e-02,\n",
      "            1.5398e-02,  7.3386e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.8539e-02, -3.5861e-02,  6.1487e-02,  ...,  1.2763e-04,\n",
      "           -3.5852e-02,  1.6202e-01]],\n",
      "\n",
      "         [[ 1.1372e-01,  3.8478e-02, -4.5383e-02,  ...,  7.7596e-02,\n",
      "           -2.4523e-01, -3.9970e-02]],\n",
      "\n",
      "         [[ 3.1365e-02, -4.3021e-02, -1.8768e-01,  ...,  1.0724e-01,\n",
      "            6.3382e-02, -9.0467e-02]]]], device='cuda:0', requires_grad=True)), (Parameter containing:\n",
      "tensor([[[[-6.2145e-01,  1.7380e+00,  9.4496e-02,  ...,  3.0168e-01,\n",
      "           -1.5209e+00, -4.7337e-01]],\n",
      "\n",
      "         [[-1.1937e+00, -1.4736e+00, -4.5591e-02,  ..., -2.8428e-01,\n",
      "           -3.5655e-01,  1.4406e+00]],\n",
      "\n",
      "         [[-1.0206e-04,  7.0567e-01,  1.4041e-01,  ...,  1.7639e+00,\n",
      "           -3.0166e-02, -1.8843e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.5664e+00,  3.7275e+00,  3.9001e-02,  ...,  2.5524e+00,\n",
      "            3.1062e+00, -3.0441e+00]],\n",
      "\n",
      "         [[ 1.0890e+00, -1.2280e+00, -4.5106e-01,  ...,  2.2313e-01,\n",
      "            1.3592e+00, -1.0765e+00]],\n",
      "\n",
      "         [[ 4.4424e-01, -2.4315e-01, -4.5695e-02,  ...,  2.0399e-01,\n",
      "            1.0879e-01, -3.7146e-01]]]], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[[[-0.0502, -0.0597, -0.0498,  ...,  0.1256, -0.0531, -0.0525]],\n",
      "\n",
      "         [[ 0.2647,  0.0284, -0.1265,  ...,  0.0086, -0.9209, -0.0705]],\n",
      "\n",
      "         [[ 0.0074, -0.0731,  0.0287,  ..., -0.0656, -0.0336,  0.0280]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0045,  0.0213,  0.0546,  ...,  0.0902,  0.1542,  0.1732]],\n",
      "\n",
      "         [[-0.0273, -0.0473, -0.0322,  ..., -0.0731,  0.0939,  0.0438]],\n",
      "\n",
      "         [[-0.0603,  0.1090,  0.0092,  ...,  0.0146, -0.0517, -0.0945]]]],\n",
      "       device='cuda:0', requires_grad=True)), (Parameter containing:\n",
      "tensor([[[[-0.3676,  0.8235,  0.2211,  ..., -0.5402, -0.0927,  1.0506]],\n",
      "\n",
      "         [[-1.0783,  1.3716,  0.6303,  ...,  0.6277, -0.1562,  0.4663]],\n",
      "\n",
      "         [[-0.1816,  0.2719, -0.8778,  ...,  0.1663, -0.2210,  1.9149]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.2429,  0.3867, -0.5862,  ..., -0.5635, -2.4558,  0.5786]],\n",
      "\n",
      "         [[-0.0845,  0.0083,  0.2882,  ..., -0.6525, -0.1184,  0.1385]],\n",
      "\n",
      "         [[-1.3792,  0.3160, -0.3778,  ...,  0.5438, -0.0200, -0.6710]]]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[[[-0.0851,  0.0686,  0.0088,  ...,  0.0575, -0.0030, -0.0784]],\n",
      "\n",
      "         [[-0.0013,  0.0032,  0.0496,  ..., -0.0354,  0.0240, -0.0179]],\n",
      "\n",
      "         [[ 0.0079, -0.1264,  0.0699,  ..., -0.0045, -0.0133, -0.0098]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0618,  0.1259,  0.0351,  ..., -0.0093, -0.1111,  0.0379]],\n",
      "\n",
      "         [[ 0.0775, -0.1900,  0.1239,  ..., -0.0191,  0.2301,  0.0339]],\n",
      "\n",
      "         [[ 0.0057, -0.0275, -0.0634,  ...,  0.0654,  0.0109, -0.0752]]]],\n",
      "       device='cuda:0', requires_grad=True)), (Parameter containing:\n",
      "tensor([[[[ 0.2532, -0.0374,  0.0441,  ..., -0.2996, -0.0929,  0.0634]],\n",
      "\n",
      "         [[-0.0573,  0.0454, -0.2626,  ...,  0.7035,  0.0354, -0.1345]],\n",
      "\n",
      "         [[ 0.4290, -0.2877, -0.0552,  ...,  0.6066,  0.2644, -0.3717]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0047,  0.0762, -0.0613,  ...,  0.1443, -0.1060, -0.0035]],\n",
      "\n",
      "         [[ 0.1916,  2.0645,  2.6922,  ..., -0.0228, -0.5494, -0.0350]],\n",
      "\n",
      "         [[-0.2238, -0.6998, -0.2083,  ..., -0.9805, -0.0272, -0.9449]]]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[[[-0.0282,  0.0378, -0.0219,  ..., -0.0956,  0.0022,  0.0143]],\n",
      "\n",
      "         [[-0.0404,  0.0033, -0.0063,  ..., -0.1910,  0.0158, -0.0390]],\n",
      "\n",
      "         [[-0.0125, -0.0156, -0.1310,  ..., -0.0257,  0.0280, -0.0891]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0136, -0.0179,  0.0141,  ..., -0.0105,  0.0388,  0.0149]],\n",
      "\n",
      "         [[-0.0345, -0.0032, -0.0928,  ...,  0.0799, -0.0096, -0.1243]],\n",
      "\n",
      "         [[ 0.0269, -0.0294,  0.0073,  ...,  0.0253,  0.0539,  0.0385]]]],\n",
      "       device='cuda:0', requires_grad=True)), (Parameter containing:\n",
      "tensor([[[[ 1.1235e-01, -1.6764e-01,  1.5239e-01,  ...,  1.5772e-02,\n",
      "           -1.9448e-04, -1.1172e-01]],\n",
      "\n",
      "         [[-4.0600e-02,  2.0385e-01,  3.7443e-01,  ..., -1.5359e-01,\n",
      "           -9.5377e-02,  1.2948e-01]],\n",
      "\n",
      "         [[-1.2049e-01,  5.9323e-01, -4.4693e-01,  ..., -3.2321e-02,\n",
      "           -3.3793e-02,  1.5582e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.6600e-01,  3.4428e-01,  3.7380e-01,  ...,  2.3384e-01,\n",
      "           -3.3032e-01, -1.5614e-01]],\n",
      "\n",
      "         [[-1.4492e+00,  1.4841e-01, -1.9235e-02,  ...,  2.2181e-01,\n",
      "            2.8955e-01, -5.2773e-01]],\n",
      "\n",
      "         [[-1.5469e-02,  2.8203e-01, -1.0199e-01,  ...,  5.5926e-01,\n",
      "            2.6348e-01,  2.2615e-01]]]], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[[[-3.3234e-02,  1.3823e-02,  2.5725e-02,  ...,  2.5953e-02,\n",
      "           -3.9039e-02,  2.9724e-02]],\n",
      "\n",
      "         [[-1.8020e-01, -2.4072e-02,  1.0186e-01,  ..., -5.8979e-02,\n",
      "            2.1697e-02, -9.6389e-02]],\n",
      "\n",
      "         [[ 9.1075e-01,  8.0676e-03, -1.5254e-02,  ...,  2.0351e+00,\n",
      "           -4.0657e-02,  9.4511e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 6.8985e-04,  4.1776e-02, -5.7193e-02,  ...,  6.5945e-02,\n",
      "           -3.2678e-02, -2.5683e-02]],\n",
      "\n",
      "         [[ 1.2271e-01, -4.0500e-02, -6.4287e-02,  ..., -3.9875e-02,\n",
      "            3.3753e-02,  5.9910e-02]],\n",
      "\n",
      "         [[ 3.7488e-02, -3.5611e-02,  9.7372e-03,  ...,  2.9729e-03,\n",
      "           -2.2525e-02, -3.1681e-03]]]], device='cuda:0', requires_grad=True)), (Parameter containing:\n",
      "tensor([[[[ 0.0527,  2.2325, -0.4190,  ..., -0.7385, -0.1301,  0.0724]],\n",
      "\n",
      "         [[-0.2068,  1.9025, -0.6362,  ...,  0.0874,  0.0712,  0.0246]],\n",
      "\n",
      "         [[ 0.5688, -0.4989, -0.3638,  ...,  0.8971,  1.0028,  0.8824]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1692, -0.3090,  0.0262,  ...,  0.0481,  0.6682, -0.5839]],\n",
      "\n",
      "         [[ 0.3817, -0.2654, -0.1404,  ...,  0.0297, -0.3422, -2.2920]],\n",
      "\n",
      "         [[-0.6734,  0.1783, -0.5731,  ...,  1.5222,  1.1721, -1.0252]]]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[[[ 0.0131,  0.0982, -0.0027,  ..., -0.0295,  0.0767, -0.0273]],\n",
      "\n",
      "         [[ 0.0035, -0.0478, -0.1469,  ..., -0.0454,  0.0270, -0.0096]],\n",
      "\n",
      "         [[ 0.0987, -0.0220, -0.0374,  ..., -0.0052, -0.0771,  0.0455]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0010,  0.0186,  0.0284,  ...,  0.0396, -0.0381, -0.0289]],\n",
      "\n",
      "         [[-0.1311, -0.0814, -0.0278,  ...,  0.1109, -0.0436,  0.1040]],\n",
      "\n",
      "         [[ 0.0576, -0.0017,  0.0237,  ..., -0.0392, -0.0066,  0.0550]]]],\n",
      "       device='cuda:0', requires_grad=True)), (Parameter containing:\n",
      "tensor([[[[ 1.4522, -0.1895, -1.2355,  ...,  0.1404, -1.0184, -0.3108]],\n",
      "\n",
      "         [[-0.4342,  0.1482,  0.0863,  ..., -0.3124,  0.4367, -0.0843]],\n",
      "\n",
      "         [[-0.1498,  0.3242, -0.7184,  ..., -0.4862, -0.7928,  0.0023]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.9578,  2.0384,  0.1804,  ...,  0.0520,  0.0285, -0.0194]],\n",
      "\n",
      "         [[ 0.5499, -0.0417, -0.0583,  ...,  0.9421,  0.1407,  0.2811]],\n",
      "\n",
      "         [[-0.2383,  0.3057, -0.3144,  ..., -0.3290, -0.0920,  0.2481]]]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[[[-0.0848, -0.0549,  0.0055,  ...,  0.0704,  0.0041,  0.0170]],\n",
      "\n",
      "         [[ 0.0004,  0.0255,  0.0045,  ...,  0.0381,  0.0238,  0.0051]],\n",
      "\n",
      "         [[-0.0445, -0.0240,  0.0029,  ...,  0.0848, -0.1031,  0.0577]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0040,  0.0350, -0.0985,  ...,  0.0074,  0.1984,  0.0746]],\n",
      "\n",
      "         [[-0.0403,  0.0062, -0.3363,  ...,  0.0167,  0.0047, -0.0208]],\n",
      "\n",
      "         [[ 0.0065,  0.0054, -0.0485,  ...,  0.0485,  0.0383, -0.1969]]]],\n",
      "       device='cuda:0', requires_grad=True)), (Parameter containing:\n",
      "tensor([[[[ 0.3386, -0.4735, -0.4949,  ..., -0.1160,  0.4750,  0.3113]],\n",
      "\n",
      "         [[-0.3272,  0.1097, -0.7575,  ...,  0.7314, -1.8506, -0.6448]],\n",
      "\n",
      "         [[-1.5044, -2.0406, -0.5361,  ..., -2.0281,  0.9703, -2.2485]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.2729, -1.5576, -0.7628,  ..., -0.6967, -0.0985,  0.0151]],\n",
      "\n",
      "         [[-0.3287, -0.6234,  0.0552,  ..., -0.5383,  0.5517, -1.1019]],\n",
      "\n",
      "         [[ 0.0829,  0.0023, -0.7645,  ...,  0.6614,  0.3462,  2.1534]]]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[[[-0.0159, -0.1185, -0.0316,  ..., -0.0209, -0.0283,  0.0275]],\n",
      "\n",
      "         [[-0.0378, -0.0629,  0.0162,  ..., -0.0434,  0.0071,  0.0063]],\n",
      "\n",
      "         [[ 0.0055,  0.0520,  0.0112,  ...,  0.0069, -0.0285, -0.0410]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1695,  0.1360,  0.1126,  ...,  0.0809, -0.1350, -0.0397]],\n",
      "\n",
      "         [[ 0.0298, -0.0881,  0.0499,  ..., -0.0922,  0.0294, -0.0405]],\n",
      "\n",
      "         [[ 0.1108,  0.0207, -0.0416,  ..., -0.0499, -0.1322, -0.0519]]]],\n",
      "       device='cuda:0', requires_grad=True)), (Parameter containing:\n",
      "tensor([[[[-0.4668, -1.1811,  0.7056,  ..., -0.0561, -0.9208,  0.2044]],\n",
      "\n",
      "         [[ 1.1420,  0.2003, -0.2702,  ..., -1.8656, -2.2469,  2.4034]],\n",
      "\n",
      "         [[ 1.8563, -0.3035, -1.2753,  ..., -1.1876, -1.1248,  0.3034]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5299, -0.3285,  0.3790,  ..., -0.7020,  0.6852,  0.7121]],\n",
      "\n",
      "         [[ 1.3243,  2.3763, -0.9775,  ..., -0.3791, -0.3683, -0.9695]],\n",
      "\n",
      "         [[-0.4954,  0.2971,  0.5066,  ..., -0.4036,  0.7418, -0.4399]]]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[[[ 0.0297, -0.0357, -0.0777,  ...,  0.0385,  0.0378, -0.0336]],\n",
      "\n",
      "         [[-0.0266,  0.0040, -0.0236,  ...,  0.0087,  0.0309, -0.0606]],\n",
      "\n",
      "         [[-0.0305,  0.0184, -0.0622,  ...,  0.0288, -0.0198,  0.0254]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0527, -0.0337,  0.0429,  ...,  0.0117,  0.1288,  0.0933]],\n",
      "\n",
      "         [[ 0.0931, -0.0864,  0.0408,  ...,  0.0681,  0.0143,  0.0021]],\n",
      "\n",
      "         [[ 0.0584, -0.1246,  0.1462,  ..., -0.0823,  0.0461,  0.0047]]]],\n",
      "       device='cuda:0', requires_grad=True)), (Parameter containing:\n",
      "tensor([[[[-1.4126, -0.3190,  1.6262,  ...,  0.1850,  2.0974, -1.4824]],\n",
      "\n",
      "         [[-1.9690,  0.1972, -0.0453,  ..., -0.7936, -0.8073,  0.2274]],\n",
      "\n",
      "         [[-0.1340, -0.4407, -0.7318,  ..., -1.2656,  0.6224, -0.7342]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.3408, -0.9601,  0.1738,  ...,  2.0957, -0.2086,  0.7371]],\n",
      "\n",
      "         [[-0.7391,  1.6354, -0.4691,  ...,  0.0608,  1.4283,  0.9413]],\n",
      "\n",
      "         [[ 1.0561, -1.3992,  0.6887,  ...,  0.7343,  2.5040, -0.8952]]]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[[[ 0.0398, -0.0286, -0.0147,  ...,  0.0369,  0.0378, -0.0038]],\n",
      "\n",
      "         [[ 0.0147,  0.0025, -0.0300,  ..., -0.0041, -0.0402,  0.0021]],\n",
      "\n",
      "         [[-0.0023,  0.0270,  0.0261,  ...,  0.0004,  0.0139,  0.0116]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0007,  0.0124,  0.0347,  ..., -0.0289, -0.0004,  0.0169]],\n",
      "\n",
      "         [[ 0.0526,  0.0118, -0.0059,  ..., -0.0294,  0.0830,  0.0784]],\n",
      "\n",
      "         [[ 0.0137,  0.0573,  0.0592,  ...,  0.0426, -0.0695, -0.0599]]]],\n",
      "       device='cuda:0', requires_grad=True)), (Parameter containing:\n",
      "tensor([[[[ 0.9453, -0.1542,  0.7242,  ...,  1.0301, -0.6424,  0.0538]],\n",
      "\n",
      "         [[ 0.4537,  1.0020, -3.2241,  ..., -1.3600,  0.6668,  0.5669]],\n",
      "\n",
      "         [[-1.9089,  2.4930,  0.6193,  ..., -0.9316, -0.2237,  0.4590]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.3181,  0.5237, -1.4517,  ..., -0.2348,  0.8924,  1.1302]],\n",
      "\n",
      "         [[ 0.2663,  1.2425, -0.5471,  ...,  0.1773,  0.1133, -0.2035]],\n",
      "\n",
      "         [[-0.0496, -0.2315,  0.1040,  ..., -0.2936, -0.0729, -0.1059]]]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[[[ 0.0229,  0.0232,  0.1026,  ...,  0.0019,  0.0094,  0.0380]],\n",
      "\n",
      "         [[-0.0431, -0.0041, -0.0409,  ..., -0.0342,  0.0706,  0.0014]],\n",
      "\n",
      "         [[-0.0082,  0.0092, -0.0549,  ...,  0.0539, -0.0173,  0.0222]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0331,  0.0264, -0.0510,  ...,  0.0483, -0.0107,  0.0421]],\n",
      "\n",
      "         [[-0.0195, -0.0266, -0.0233,  ...,  0.0268, -0.0614,  0.0127]],\n",
      "\n",
      "         [[-0.0187,  0.0342, -0.0476,  ..., -0.0284, -0.0056,  0.0210]]]],\n",
      "       device='cuda:0', requires_grad=True)), (Parameter containing:\n",
      "tensor([[[[ 0.2146, -0.5507, -0.0430,  ...,  0.1915, -0.7650, -0.3336]],\n",
      "\n",
      "         [[-0.4885,  0.1792, -0.6038,  ..., -0.0701,  0.4416,  2.0796]],\n",
      "\n",
      "         [[ 0.4938, -1.1834,  1.0921,  ...,  0.6633, -0.1836,  0.0463]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.7497,  1.9579, -1.5964,  ...,  0.5093,  1.6208, -2.7803]],\n",
      "\n",
      "         [[ 0.7841,  0.9525,  1.7206,  ...,  1.8975, -0.5787,  1.0113]],\n",
      "\n",
      "         [[-0.5681, -0.3022, -0.1129,  ..., -0.2153,  0.4216,  0.1450]]]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[[[-0.0320,  0.0216,  0.0550,  ...,  0.0479,  0.0875,  0.0304]],\n",
      "\n",
      "         [[-0.0267, -0.1311, -0.0594,  ...,  0.0816, -0.0665, -0.1238]],\n",
      "\n",
      "         [[-0.0079, -0.0391, -0.0210,  ..., -0.0332,  0.0070, -0.1281]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0412, -0.0372,  0.0256,  ..., -0.0164,  0.0011, -0.0434]],\n",
      "\n",
      "         [[ 0.0203,  0.0326,  0.0648,  ..., -0.0063,  0.0996,  0.0388]],\n",
      "\n",
      "         [[-0.0645, -0.0149,  0.0466,  ...,  0.0377, -0.0051, -0.0117]]]],\n",
      "       device='cuda:0', requires_grad=True)), (Parameter containing:\n",
      "tensor([[[[ 0.9339,  2.0062, -0.1250,  ...,  0.5244, -0.5404, -0.7605]],\n",
      "\n",
      "         [[-0.0189,  0.0336, -0.1025,  ..., -0.4120,  0.8273,  0.1632]],\n",
      "\n",
      "         [[-0.2562,  0.3591,  0.5362,  ...,  0.9401,  0.2685, -3.3129]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.6931, -0.2863, -1.6150,  ..., -0.3004,  0.1042,  1.3369]],\n",
      "\n",
      "         [[-0.4191, -1.3659, -1.1174,  ..., -1.3952, -0.7256,  2.1036]],\n",
      "\n",
      "         [[-0.7989, -0.0894, -0.3845,  ...,  2.2197,  2.6034,  0.1840]]]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[[[ 3.9831e-02, -2.0039e-02,  4.2082e-02,  ...,  3.9567e-02,\n",
      "            2.1018e-02,  2.0384e-04]],\n",
      "\n",
      "         [[ 1.5334e-01,  9.0832e-02,  4.2551e-02,  ..., -1.3684e-01,\n",
      "           -4.3838e-02,  8.1879e-02]],\n",
      "\n",
      "         [[-4.3806e-02, -1.1365e-03, -1.9545e-02,  ..., -3.8147e-03,\n",
      "            7.5340e-03,  2.6108e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.6869e-02,  3.2796e-02, -2.6088e-02,  ..., -2.1013e-01,\n",
      "           -3.5240e-02,  6.3877e-02]],\n",
      "\n",
      "         [[-5.1829e-02, -1.5022e-01, -2.0641e-02,  ..., -4.0560e-02,\n",
      "           -2.9653e-02,  1.1588e-01]],\n",
      "\n",
      "         [[-2.3175e-02, -3.9371e-02, -9.9815e-02,  ...,  1.1723e-02,\n",
      "           -6.9018e-02,  2.7637e-02]]]], device='cuda:0', requires_grad=True)), (Parameter containing:\n",
      "tensor([[[[ 0.2685,  0.6303, -2.1174,  ...,  0.4745,  0.3424, -0.4068]],\n",
      "\n",
      "         [[-0.2926,  0.0333, -0.0059,  ...,  0.2581, -0.0573,  0.1547]],\n",
      "\n",
      "         [[-0.0996,  1.0687,  0.4940,  ..., -0.3476,  1.2689, -0.7469]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0805,  0.2495, -0.5487,  ...,  1.0360,  1.6491, -0.6085]],\n",
      "\n",
      "         [[-1.1031, -0.0514, -0.1152,  ..., -0.2800, -0.1992, -0.9235]],\n",
      "\n",
      "         [[-0.6285, -1.7211,  0.7463,  ...,  1.5033,  0.9222,  0.7562]]]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[[[ 0.0171,  0.0391,  0.0028,  ...,  0.0462, -0.0294, -0.0531]],\n",
      "\n",
      "         [[ 0.0093, -0.0131, -0.0114,  ..., -0.0047,  0.0210,  0.0513]],\n",
      "\n",
      "         [[-0.0705, -0.0759,  0.0442,  ...,  0.0283,  0.0193,  0.0041]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0029, -0.0101,  0.0237,  ..., -0.0078, -0.0414, -0.0030]],\n",
      "\n",
      "         [[-0.0097, -0.0273, -0.0964,  ..., -0.0500, -0.0154, -0.0110]],\n",
      "\n",
      "         [[ 0.0156, -0.0122, -0.0033,  ..., -0.0073,  0.0204, -0.0441]]]],\n",
      "       device='cuda:0', requires_grad=True)), (Parameter containing:\n",
      "tensor([[[[-0.0315,  0.1244,  0.0509,  ..., -0.0296,  0.0479, -0.0308]],\n",
      "\n",
      "         [[ 0.0208,  0.3552,  0.2413,  ..., -0.5188, -0.0758,  1.4943]],\n",
      "\n",
      "         [[ 0.2678, -0.6940, -1.1515,  ...,  0.0433, -0.5219, -0.9627]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.5541,  0.4133, -1.9695,  ..., -0.2584, -1.1863,  0.6370]],\n",
      "\n",
      "         [[ 0.7874,  0.0983,  0.3511,  ..., -0.4596, -0.1130,  1.3637]],\n",
      "\n",
      "         [[-1.3579, -1.6171, -0.0296,  ...,  1.6261,  0.8765,  0.4292]]]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[[[-0.0288, -0.0833, -0.0130,  ..., -0.0768,  0.1181,  0.1079]],\n",
      "\n",
      "         [[-0.0461,  0.0283, -0.0447,  ...,  0.0147,  0.0448,  0.0568]],\n",
      "\n",
      "         [[ 0.0270,  0.0248, -0.0319,  ..., -0.0228, -0.0236,  0.0131]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0265, -0.0110,  0.0004,  ...,  0.0201, -0.0724,  0.0698]],\n",
      "\n",
      "         [[ 0.0171, -0.0067, -0.0112,  ..., -0.0283,  0.0234, -0.0256]],\n",
      "\n",
      "         [[ 0.0322,  0.0031, -0.0201,  ..., -0.0278,  0.0292, -0.0057]]]],\n",
      "       device='cuda:0', requires_grad=True)), (Parameter containing:\n",
      "tensor([[[[ 0.3180,  0.4601,  0.8009,  ...,  0.6731,  0.4340, -1.0762]],\n",
      "\n",
      "         [[-0.1770, -0.3735, -0.1264,  ...,  0.5229, -0.6175, -0.2509]],\n",
      "\n",
      "         [[ 0.1406, -0.3374,  0.2432,  ...,  0.3710, -0.3200,  0.3092]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.2797, -0.7320, -0.4819,  ...,  0.5531,  0.8158,  0.2223]],\n",
      "\n",
      "         [[ 0.1626, -0.2039,  0.1125,  ..., -0.1868,  0.1040, -0.6352]],\n",
      "\n",
      "         [[-0.9894,  0.7039, -0.5465,  ...,  1.1766,  0.6517,  1.0663]]]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[[[-0.0201,  0.0220, -0.1007,  ...,  0.0426, -0.0538,  0.0415]],\n",
      "\n",
      "         [[-0.0428,  0.0420,  0.0049,  ..., -0.0237, -0.0166,  0.0342]],\n",
      "\n",
      "         [[ 0.0207, -0.0596,  0.0512,  ...,  0.0228,  0.0544, -0.0095]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0524,  0.0249,  0.0244,  ...,  0.0111, -0.0680, -0.0001]],\n",
      "\n",
      "         [[-0.0012,  0.0323,  0.0046,  ...,  0.0012, -0.0496, -0.0632]],\n",
      "\n",
      "         [[ 0.0575,  0.0171, -0.0108,  ..., -0.0613,  0.0611,  0.0313]]]],\n",
      "       device='cuda:0', requires_grad=True)), (Parameter containing:\n",
      "tensor([[[[-0.5075,  0.5452, -0.0600,  ...,  0.3643,  0.8055, -0.9398]],\n",
      "\n",
      "         [[ 0.1701, -2.0430,  0.0734,  ..., -0.0579, -0.2481,  0.0893]],\n",
      "\n",
      "         [[ 0.2895,  0.3320, -0.0273,  ...,  0.4626,  0.5430,  0.1964]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9748,  0.2447,  0.2905,  ..., -0.6561,  0.3114,  0.4709]],\n",
      "\n",
      "         [[-0.0306,  0.2647,  0.0266,  ...,  0.2498, -0.5504, -0.1752]],\n",
      "\n",
      "         [[-0.3782, -1.6048,  0.0093,  ..., -0.5674,  0.3674, -0.7324]]]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[[[-0.0064,  0.0030, -0.0769,  ...,  0.0329, -0.0103,  0.0344]],\n",
      "\n",
      "         [[ 0.0224,  0.0182, -0.0484,  ..., -0.0113,  0.0375,  0.0150]],\n",
      "\n",
      "         [[-0.0200, -0.1381, -0.0437,  ...,  0.0212,  0.0388,  0.0658]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0258,  0.0934, -0.0864,  ..., -0.0370, -0.1070, -0.1253]],\n",
      "\n",
      "         [[ 0.0586,  0.1683, -0.0517,  ..., -0.0605,  0.0152,  0.0516]],\n",
      "\n",
      "         [[-0.0087, -0.0241,  0.0159,  ...,  0.0190, -0.0579, -0.0756]]]],\n",
      "       device='cuda:0', requires_grad=True)), (Parameter containing:\n",
      "tensor([[[[-2.1902e-01, -1.2854e-01,  3.9445e-01,  ...,  2.9167e-01,\n",
      "           -3.4094e-01, -2.7493e-01]],\n",
      "\n",
      "         [[-2.8387e-01, -1.5775e-03,  6.8439e-02,  ..., -2.2189e-02,\n",
      "           -6.3842e-02, -1.0428e+00]],\n",
      "\n",
      "         [[-7.0962e-01,  2.1179e+00,  5.3458e-01,  ..., -2.2612e-01,\n",
      "            1.0915e-01,  2.2172e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 8.4032e-04, -4.1135e-01,  9.0542e-02,  ..., -4.4213e-01,\n",
      "            3.6694e-01,  2.3883e-01]],\n",
      "\n",
      "         [[ 4.4109e-04, -7.3828e-01, -7.6022e-02,  ...,  2.6208e-01,\n",
      "            1.2722e-01,  1.8252e-01]],\n",
      "\n",
      "         [[ 4.9662e-02,  1.9655e-01, -1.0783e-01,  ...,  2.0120e-03,\n",
      "           -1.9179e-01,  8.5746e-01]]]], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[[[ 4.7327e-02, -3.1074e-02,  4.7238e-02,  ..., -3.7653e-02,\n",
      "            1.0155e-01,  3.4237e-02]],\n",
      "\n",
      "         [[-1.4429e-01,  1.1255e-01, -1.8144e-01,  ...,  1.5783e-01,\n",
      "           -6.1843e-02,  1.3018e-01]],\n",
      "\n",
      "         [[ 8.0559e-02,  3.9573e-02, -3.4152e-02,  ...,  2.0169e-02,\n",
      "           -6.6857e-03,  1.0911e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.3816e-02,  6.5339e-02, -3.3884e-02,  ...,  4.3189e-02,\n",
      "           -5.5412e-04, -5.5589e-02]],\n",
      "\n",
      "         [[-3.9485e-02, -7.2675e-03, -3.6483e-02,  ..., -5.0133e-02,\n",
      "            4.4388e-02,  3.9289e-03]],\n",
      "\n",
      "         [[ 7.2169e-02, -1.1067e-04, -5.2200e-02,  ...,  1.1642e-02,\n",
      "            5.4478e-02,  1.9620e-02]]]], device='cuda:0', requires_grad=True)), (Parameter containing:\n",
      "tensor([[[[-0.2407, -0.1591,  2.1260,  ...,  0.2030,  0.4097, -0.2476]],\n",
      "\n",
      "         [[ 1.3277,  0.9264,  1.5403,  ...,  1.2758, -1.3470, -0.8073]],\n",
      "\n",
      "         [[ 0.2169,  0.1828, -0.2136,  ...,  0.0291, -0.2517, -0.0553]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0363, -2.1090, -0.0062,  ..., -0.2699,  0.5232,  0.3653]],\n",
      "\n",
      "         [[-0.0402,  0.1382,  0.7179,  ...,  1.3158,  0.0131, -0.2662]],\n",
      "\n",
      "         [[-0.3522,  0.5543, -0.6913,  ..., -0.1293,  0.4694, -0.7296]]]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[[[ 0.0132,  0.0823, -0.0959,  ...,  0.1561,  0.0198, -0.0510]],\n",
      "\n",
      "         [[ 0.0086,  0.0535,  0.0766,  ...,  0.0299,  0.0256, -0.0352]],\n",
      "\n",
      "         [[ 0.0025, -0.0588,  0.0065,  ..., -0.0850,  0.0695, -0.0050]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0547, -0.0502, -0.0872,  ..., -0.0472, -0.0019, -0.0419]],\n",
      "\n",
      "         [[-0.0448, -0.0017,  0.0214,  ..., -0.0067,  0.0470,  0.0393]],\n",
      "\n",
      "         [[ 0.0282, -0.0246, -0.0094,  ...,  0.0144, -0.0583,  0.0626]]]],\n",
      "       device='cuda:0', requires_grad=True)), (Parameter containing:\n",
      "tensor([[[[ 0.0334, -0.1013,  0.1615,  ...,  2.2597,  0.1258,  0.2833]],\n",
      "\n",
      "         [[-0.0079, -0.0874,  2.2229,  ...,  0.0804, -0.3415, -0.3988]],\n",
      "\n",
      "         [[-0.2614, -0.3518,  0.3677,  ...,  0.0926,  0.4606,  0.3251]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1788,  0.4712,  0.6674,  ...,  0.1746, -0.3237,  0.2020]],\n",
      "\n",
      "         [[-0.6722,  0.3029,  0.0508,  ..., -0.8878, -0.3582, -0.0466]],\n",
      "\n",
      "         [[-0.1452, -0.5192,  0.1227,  ...,  0.3900,  0.0452, -0.3122]]]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[[[-0.0400,  0.0883, -0.0088,  ..., -0.0214, -0.0142, -0.0125]],\n",
      "\n",
      "         [[-0.1751, -0.0276,  0.1082,  ..., -0.0492, -0.0134, -0.0122]],\n",
      "\n",
      "         [[ 0.0893,  0.0346,  0.2311,  ...,  0.0369, -0.1510, -0.0308]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0484, -0.1799, -0.0142,  ..., -0.0264, -0.1719,  0.0826]],\n",
      "\n",
      "         [[-0.0096,  0.5085,  0.2513,  ...,  0.0384,  0.0033,  0.0550]],\n",
      "\n",
      "         [[ 0.0209,  0.0746, -0.0390,  ..., -0.0115, -0.5662,  0.1142]]]],\n",
      "       device='cuda:0', requires_grad=True)), (Parameter containing:\n",
      "tensor([[[[ 0.2039,  0.5541, -0.0156,  ...,  0.9706,  0.1896, -1.0808]],\n",
      "\n",
      "         [[-0.4258, -0.4923, -0.3417,  ...,  0.1112, -0.0352,  0.2810]],\n",
      "\n",
      "         [[ 0.6404,  0.3773,  2.8374,  ...,  0.1277, -0.8558, -0.3929]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.2244, -0.3366,  0.5492,  ...,  0.5401,  0.0175,  0.1046]],\n",
      "\n",
      "         [[-0.1087, -0.1446, -0.1351,  ..., -0.5473, -0.3286, -0.2622]],\n",
      "\n",
      "         [[-0.5161,  0.2916,  0.3612,  ...,  0.4402,  0.1353, -0.0866]]]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[[[ 0.0994,  0.0246, -0.0549,  ...,  0.0253, -0.0324, -0.0962]],\n",
      "\n",
      "         [[-0.0430,  0.2382,  0.1456,  ..., -0.0147, -0.0867, -0.0545]],\n",
      "\n",
      "         [[ 0.5399,  0.5668, -0.1261,  ..., -0.2830, -0.0445,  0.1127]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0095,  0.0081,  0.2087,  ...,  0.0040,  0.1913, -0.0214]],\n",
      "\n",
      "         [[ 0.0853,  0.1344, -0.0094,  ..., -0.0051, -0.1389,  0.0333]],\n",
      "\n",
      "         [[-0.3415, -0.2893, -0.2192,  ...,  0.0480,  0.1247,  0.3484]]]],\n",
      "       device='cuda:0', requires_grad=True)))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "total average loss: 2.608179\n"
     ]
    }
   ],
   "source": [
    "# for debugging in notebook\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "model.zero_grad()\n",
    "\n",
    "\n",
    "loss_per_update = 0\n",
    "total_loss = 0\n",
    "\n",
    "# if reset_pos_emb:\n",
    "#     if all_padding_length is None:\n",
    "#         c_position_ids = torch.arange(1, 1 + context_t[:, 1:-1].shape[-1], dtype=torch.long, device=device).unsqueeze(0).repeat(batch_size, 1)\n",
    "#     else:\n",
    "#         batch_max_length = max(all_padding_length)\n",
    "#         all_c_position_ids = list()\n",
    "#         for padding_length in all_padding_length:\n",
    "#             ci_position_ids = torch.cat((torch.zeros(padding_length + 1, dtype=torch.long), torch.arange(1, batch_max_length - padding_length - 1, dtype=torch.long))).unsqueeze(0).to(device)\n",
    "#             all_c_position_ids.append(ci_position_ids)\n",
    "#         c_position_ids = torch.tensor(all_c_position_ids)    \n",
    "# else:\n",
    "#     c_position_ids = None\n",
    "if reset_pos_emb:\n",
    "    c_position_ids = torch.arange(1, batch_min_length - 1, dtype=torch.long, device=device)\n",
    "    c_position_ids = c_position_ids.unsqueeze(0).repeat(batch_size, 1)\n",
    "else:\n",
    "    c_position_ids = None\n",
    "\n",
    "if reset_pos_emb:\n",
    "    t_position_ids = torch.ones(batch_size, num_of_triggers).to(torch.long).to(device) * TRIGGER_POSITION_ID\n",
    "else:\n",
    "    t_position_ids = None\n",
    "    \n",
    "# print(\"c_position; t_position\")\n",
    "# print(c_position_ids)\n",
    "# print(t_position_ids)\n",
    "# print()\n",
    "\n",
    "# print(\"all_inputs; batch_min_length\")\n",
    "# print(all_input_ids)\n",
    "# print(batch_min_length)\n",
    "# print()\n",
    "\n",
    "all_context_prompts = []\n",
    "all_generated_prompt_length = []\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    \n",
    "    past = lm_bos_output[\"past_key_values\"]\n",
    "    \n",
    "    # print(\"bos past key value\")\n",
    "    # print(past[-1][0][0, 0, :, :20])\n",
    "    # print(past[-1][1][0, 0, :, :20])\n",
    "    if num_of_triggers > 0:\n",
    "        if trigger_format == \"token\":\n",
    "#             lm_trigger_output = model(inputs_embeds=trigger_embedding, past_key_values=past, position_ids=t_position_ids)\n",
    "            trigger_embedding = model.ori_trigger_embedding.repeat(batch_size, 1, 1)\n",
    "            lm_trigger_output = model(inputs_embeds=trigger_embedding, position_ids=t_position_ids)\n",
    "            trigger_key_values = lm_trigger_output[\"past_key_values\"]\n",
    "        else:\n",
    "            trigger_key_values = expand_past(model.ori_trigger_key_values, num_layers, batch_size)\n",
    "        past = concat_past(past, trigger_key_values, num_layers)\n",
    "\n",
    "        # print(\"bos + trigger past key value\")\n",
    "        # print(past[-1][0][0, 0, :, :20])\n",
    "        # print(past[-1][1][0, 0, :, :20])\n",
    "    else:\n",
    "        trigger_key_values = None\n",
    "\n",
    "\n",
    "    output_so_far = all_input_ids[:, :batch_min_length]  # bze x (batch_min_length - 1)\n",
    "        \n",
    "#     context_lm_output = model(context_t[:, 1:-1], past_key_values=past, position_ids=c_position_ids, attention_mask=all_attnention_masks)\n",
    "    context_lm_output = model(all_input_ids[:, 1: batch_min_length - 1], past_key_values=past, position_ids=c_position_ids, )\n",
    "    \n",
    "    past = context_lm_output[\"past_key_values\"]\n",
    "    last = output_so_far[:, batch_min_length - 1: batch_min_length]\n",
    "    \n",
    "#     print(\"checking contexts\")\n",
    "#     print(output_so_far)\n",
    "#     print(all_input_ids[:, 1: batch_min_length - 1])\n",
    "#     print(last)\n",
    "#     print()\n",
    "    \n",
    "    # optimizer.zero_grad()\n",
    "\n",
    "    gumbel_vector = None\n",
    "    if detach:\n",
    "#         all_gumbel_vectors = [[] for _ in range(batch_size)]\n",
    "        all_gumbel_vectors = None\n",
    "    \n",
    "    prompt_not_done = torch.ones(batch_size, 1, dtype=torch.uint8, device=device)\n",
    "    generated_prompt_length = torch.zeros(batch_size, 1, dtype=torch.long, device=device)\n",
    "    prompt_stop_first = True\n",
    "\n",
    "    # generate conditional prompt\n",
    "    print(\"=====Iteration: %d=====\" % (i + 1))\n",
    "#     print(\"^^^\"*50)\n",
    "    for p_i in range(length):\n",
    "        if reset_pos_emb:\n",
    "            past_length = past[0][0].size(-2)\n",
    "            p_position_ids = torch.arange(past_length - num_of_triggers, past_length - num_of_triggers + 1, dtype=torch.long, device=device)\n",
    "            p_position_ids = p_position_ids.unsqueeze(0).repeat(batch_size, 1)\n",
    "        else:\n",
    "            p_position_ids = None\n",
    "        if gumbel_softmax and gumbel_vector is not None:\n",
    "            # before (single batch)\n",
    "            # last_emb = torch.mm(gumbel_vector, model.transformer.wte.weight).unsqueeze(0)  # needs to be bze, n, emb\n",
    "            # lm_output = model(inputs_embeds=last_emb, past_key_values=past, position_ids=p_position_ids)\n",
    "            last_emb = torch.mm(gumbel_vector, model.transformer.wte.weight).unsqueeze(1)  # needs to be bze, n, emb\n",
    "            lm_output = model(inputs_embeds=last_emb, past_key_values=past, position_ids=p_position_ids)\n",
    "        else:\n",
    "            lm_output = model(last, past_key_values=past, position_ids=p_position_ids)\n",
    "            \n",
    "#         print(\"p_position_ids\")\n",
    "#         print(p_position_ids)\n",
    "#         print()\n",
    "            \n",
    "        logits, past, all_hidden = (\n",
    "            lm_output[\"logits\"],  # bze, cur_seq_len, vocab_size\n",
    "            lm_output[\"past_key_values\"],  # acc_seq_len\n",
    "            lm_output[\"hidden_states\"],  # num_layers + 1, tuple of (bze, cur_seq_len, hid_sze)\n",
    "        )\n",
    "        \n",
    "        vocab_size = logits.shape[-1]\n",
    "        \n",
    "        logits = penalize_new_line(logits)\n",
    "        \n",
    "        # last: bze x 1; gumbel_vector: bze x vocab_size\n",
    "        last, gumbel_vector = generate_next(logits, output_so_far, top_k=top_k, temperature=temperature, \n",
    "                                            repetition_penalty=repetition_penalty, sample=sample, \n",
    "                                            gumbel_softmax=gumbel_softmax, gumbel_temperature=gumbel_temperature, detach=detach)\n",
    "        # manually assign end token is too long\n",
    "        if p_i == length - 1:\n",
    "            for m_b_i in range(batch_size):\n",
    "                if generated_prompt_length[m_b_i] == 0:\n",
    "                    last[m_b_i] = tokenizer.encode(\".\")[0]  # encode outputs a list (1 element)\n",
    "                    gumbel_vector[m_b_i] = F.one_hot(torch.tensor(tokenizer.encode(\".\"), dtype=torch.long, device=device), num_classes=vocab_size)\n",
    "        \n",
    "        # generate one sentence which ends with \".\" as the prompt\n",
    "#         if last.squeeze(0).data.cpu().numpy()[0] == tokenizer.encode(\".\")[0]:\n",
    "#             break\n",
    "        \n",
    "        # double check the length (p_i vs. lengths below)\n",
    "        is_generated = torch.tensor(all_lengths, device=device).unsqueeze(-1) <= (p_i + batch_min_length)  # bze x 1. is generated or stil in the context\n",
    "        is_end_token = last == torch.tensor(tokenizer.encode(\".\"), device=device)  # bze x 1\n",
    "        is_actually_ending = is_generated * is_end_token\n",
    "#         print(\"is_generated; is_end_token; is_actually_ending\")\n",
    "#         print(is_generated)\n",
    "#         print(is_end_token)\n",
    "#         print(is_actually_ending)\n",
    "#         print()\n",
    "        # keep track of prompt length\n",
    "        generated_prompt_length = generated_prompt_length + prompt_not_done * is_actually_ending * p_i\n",
    "#         print(\"generated_prompt_length\")\n",
    "#         print(generated_prompt_length)\n",
    "#         print()\n",
    "        \n",
    "        # if generated, use the generated token as last; otherwise (from the original), copy the orignal token/gumbel_vector\n",
    "#         print(\"last\\n: before\")\n",
    "#         print(last)\n",
    "#         for li in last:\n",
    "#             print(tokenizer.decode(li))\n",
    "        # last = last * is_generated + all_input_ids[:, batch_min_length + p_i].unsqueeze(1) * (1 - is_generated)\n",
    "        if batch_min_length + p_i < all_input_ids.shape[1]:\n",
    "            last = last * is_generated + all_input_ids[:, batch_min_length + p_i].unsqueeze(1) * (~is_generated)  # is_generated is bool. need to use \"~\" instead of (1-is_generated)\n",
    "        else:\n",
    "            last = last\n",
    "#         print(\"after\")\n",
    "#         print(last)\n",
    "#         for li in last:\n",
    "#             print(tokenizer.decode(li))\n",
    "#         print()\n",
    "        \n",
    "#         for btbi in range(batch_size):\n",
    "#             print(tokenizer.decode(output_so_far[btbi].tolist()))\n",
    "#         print()\n",
    "        \n",
    "        if gumbel_softmax:\n",
    "            if batch_min_length + p_i < all_input_ids.shape[1]:\n",
    "                ori_one_hot = F.one_hot(all_input_ids[:, batch_min_length + p_i], num_classes=vocab_size)  # bze x vocab_size \n",
    "                # gumbel_vector = gumbel_vector * is_generated + ori_one_hot * (1 - is_generated)\n",
    "                gumbel_vector = gumbel_vector * is_generated + ori_one_hot * (~is_generated)\n",
    "            else:\n",
    "                gumbel_vector = gumbel_vector\n",
    "        if prompt_stop_first and torch.sum(is_actually_ending) > 0:\n",
    "            prompt_stop_first = False\n",
    "            min_p_past = past\n",
    "            min_p_last = last\n",
    "            min_gumbel_vector = gumbel_vector\n",
    "        \n",
    "        if detach: \n",
    "            # all_gumbel_vectors.append(gumbel_vector)\n",
    "            \n",
    "            # WARNING: This can be quite large\n",
    "            if all_gumbel_vectors is None:\n",
    "                all_gumbel_vectors = gumbel_vector.unsqueeze(1)  # bze x 1 x vocab_size\n",
    "            else:\n",
    "                all_gumbel_vectors = torch.cat((all_gumbel_vectors, gumbel_vector.unsqueeze(1)), dim=1)  # bze x n x vocab_size\n",
    "        \n",
    "#         print(\"***\"*20)\n",
    "#         print(output_so_far.shape)\n",
    "#         print(last.shape)\n",
    "        output_so_far = torch.cat((output_so_far, last), dim=1)  # bze x length\n",
    "        \n",
    "#         print(\"prompt_not_done\\n: before\")\n",
    "#         print(prompt_not_done)\n",
    "        # prompt_not_done = prompt_not_done * (1 - is_actually_ending)  # to check is we need to stop by summing\n",
    "        prompt_not_done = prompt_not_done * (~is_actually_ending)  # to check is we need to stop by summing\n",
    "#         print(\"after\")\n",
    "#         print(prompt_not_done)\n",
    "#         print()\n",
    "        if torch.sum(prompt_not_done) == 0:\n",
    "            break\n",
    "        \n",
    "    \n",
    "    print(\"context + prompt\")\n",
    "    # print(output_so_far.tolist())\n",
    "#     print(tokenizer.decode(output_so_far.tolist()[0]))\n",
    "#     print(\"output_so_far\")\n",
    "#     print(output_so_far)\n",
    "#     print(generated_prompt_length.squeeze(-1).tolist())\n",
    "    batch_context_prompts = []\n",
    "    for cp_i in range(batch_size):\n",
    "        cp_i_text = tokenizer.decode(output_so_far[cp_i][:(generated_prompt_length[cp_i].item() + 1 + batch_min_length)].tolist())\n",
    "        print(cp_i_text)\n",
    "        batch_context_prompts.append(cp_i_text)\n",
    "    print(\"***\" * 20)\n",
    "#     assert False\n",
    "#     print(\"========\"*20)\n",
    "    all_context_prompts.append(batch_context_prompts)\n",
    "    all_generated_prompt_length.append(generated_prompt_length + batch_min_length + 1)\n",
    "    \n",
    "    cp_min_length = torch.min(generated_prompt_length.squeeze(1)) + batch_min_length + 1\n",
    "    \n",
    "    if detach:\n",
    "#         detach_context_output = model(context_t)  # Note: context + prompt here will not attend to the trigger(s)\n",
    "#         past = detach_context_output[\"past_key_values\"]\n",
    "#         for last_detach_vector in all_gumbel_vectors[:-1]:  # the last gumbel vector is \".\"\n",
    "#             last_detach_emb = torch.mm(last_detach_vector, model.transformer.wte.weight).unsqueeze(0)\n",
    "#             last_detach_output = model(inputs_embeds=last_detach_emb, past_key_values=past)\n",
    "#             past = last_detach_output[\"past_key_values\"]\n",
    "        detach_context_output = model(all_input_ids[:, :batch_min_length])\n",
    "        past = detach_context_output[\"past_key_values\"]\n",
    "#         print(\"detach context past\")\n",
    "#         print(all_input_ids[:, :batch_min_length])\n",
    "#         print(past[0][0].shape)\n",
    "        all_gumbel_embeddings = torch.matmul(all_gumbel_vectors[:, :cp_min_length - batch_min_length - 1, :], model.transformer.wte.weight)\n",
    "        last_detach_output = model(inputs_embeds=all_gumbel_embeddings, past_key_values=past)\n",
    "        past = last_detach_output[\"past_key_values\"]\n",
    "#         print(\"detach all gumbel embeddings past\")\n",
    "#         print(past[0][0].shape)\n",
    "#         print(\"min_past\")\n",
    "#         print(min_p_past[0][0].shape)\n",
    "        gumbel_vector = min_gumbel_vector\n",
    "#         print(\"checking gumbel vector\")\n",
    "#         print(torch.max(gumbel_vector, dim=-1))\n",
    "#         print(torch.max(all_gumbel_vectors[:, cp_min_length - batch_min_length - 1: cp_min_length - batch_min_length, :].squeeze(1), dim=-1))\n",
    "#         assert gumbel_vector == all_gumbel_vectors[:, cp_min_length - batch_min_length - 1: cp_min_length - batch_min_length, :].squeeze(1)\n",
    "    else:\n",
    "        # adjust past, last, and gumbel_vector\n",
    "        past = min_p_past\n",
    "        last = min_p_last\n",
    "\n",
    "    \n",
    "    everything_so_far = output_so_far[:, :cp_min_length]\n",
    "#     print(\"everything_so_far\")\n",
    "#     for cp_i in range(batch_size):\n",
    "#         print(tokenizer.decode(everything_so_far[cp_i].tolist()))\n",
    "#     print()\n",
    "    \n",
    "    # generate response\n",
    "    response_hidden = None\n",
    "#     response_last_ck = None\n",
    "    response_so_far = None\n",
    "    first = True\n",
    "    to_break = False\n",
    "    \n",
    "    response_not_done = torch.ones(batch_size, 1, dtype=torch.uint8, device=device)\n",
    "    generated_response_length = torch.zeros(batch_size, 1, dtype=torch.long, device=device)\n",
    "    response_stop_first = True\n",
    "    \n",
    "    for r_i in range(length):\n",
    "        # TODO: mask trigger key and value\n",
    "\n",
    "        if num_of_triggers > 0 and not not_mask_trigger and not detach:\n",
    "            # create attention mask\n",
    "            past_length = past[0][0].shape[-2]\n",
    "            attention_mask = torch.ones(batch_size, past_length + 1)  # add current 1 to length\n",
    "            attention_mask[:, 1: 1 + num_of_triggers] = 0  # bze=1, the first element is BOS\n",
    "            attention_mask = attention_mask.to(device)\n",
    "        else:\n",
    "            attention_mask = None\n",
    "\n",
    "        # lm_rep_output = model(last, past_key_values=past, attention_mask=attention_mask)\n",
    "\n",
    "        if num_of_triggers > 0 and reset_pos_emb and not detach:\n",
    "            past_length = past[0][0].size(-2)\n",
    "            r_position_ids = torch.arange(past_length - num_of_triggers, past_length - num_of_triggers + 1, dtype=torch.long, device=device)\n",
    "            r_position_ids = r_position_ids.unsqueeze(0).repeat(batch_size, 1)\n",
    "        else:\n",
    "            r_position_ids = None\n",
    "            \n",
    "#         print(\"r_position_ids\")\n",
    "#         print(r_position_ids)\n",
    "#         print(torch.arange(min_p_past[0][0].shape[-2] - num_of_triggers, min_p_past[0][0].shape[-2] - num_of_triggers + 1, dtype=torch.long, device=device))\n",
    "#         print()\n",
    "        \n",
    "#         print(\"debugging min_past vs. gumbel past\")\n",
    "#         print(\"min_p\")\n",
    "#         print(min_p_last)\n",
    "#         print(min_p_past[0][0].shape)  # includes trigger (so longer than gumbel past)\n",
    "#         print(\"gumbel\")\n",
    "#         print(past[0][0].shape)\n",
    "#         print()\n",
    "\n",
    "        # debugging\n",
    "        if gumbel_softmax:\n",
    "            last_emb = torch.mm(gumbel_vector, model.transformer.wte.weight).unsqueeze(1)  # bze x 1 x hidden\n",
    "            lm_rep_output = model(inputs_embeds=last_emb, past_key_values=past, attention_mask=attention_mask,\n",
    "                              output_attentions=True, position_ids=r_position_ids)\n",
    "        else:\n",
    "            lm_rep_output = model(last, past_key_values=past, attention_mask=attention_mask, output_attentions=True,\n",
    "                              position_ids=r_position_ids)\n",
    "\n",
    "        rep_logits, past, rep_all_hidden = (\n",
    "            lm_rep_output[\"logits\"],  # bze, cur_seq_len, vocab_size\n",
    "            lm_rep_output[\"past_key_values\"],  # acc_seq_len\n",
    "            lm_rep_output[\"hidden_states\"],  # num_layers + 1, tuple of (bze, cur_seq_len, hid_sze)\n",
    "        )\n",
    "        \n",
    "#         if response_so_far is not None:\n",
    "#             everything_so_far = output_so_far[0].tolist() + response_so_far[0].tolist()\n",
    "#         else:\n",
    "#             everything_so_far = output_so_far[0].tolist()\n",
    "        \n",
    "        rep_logits = penalize_new_line(rep_logits)\n",
    "            \n",
    "        last, gumbel_vector = generate_next(rep_logits, everything_so_far, top_k=top_k, temperature=temperature, \n",
    "                                            repetition_penalty=repetition_penalty, sample=sample, \n",
    "                                            gumbel_softmax=gumbel_softmax, gumbel_temperature=gumbel_temperature, detach=detach)\n",
    "\n",
    "        last_hidden = rep_all_hidden[-1]\n",
    "\n",
    "#         if first:\n",
    "#             first = False\n",
    "#         else:\n",
    "#             response_hidden.append(last_hidden)\n",
    "#             # print(tokenizer.decode(last.tolist()[0]))\n",
    "    \n",
    "#         response_hidden.append(last_hidden)  # [(bze, 1, hid_size)]\n",
    "#         response_last_ck.append(last)\n",
    "        if response_hidden is None:\n",
    "            response_hidden = last_hidden\n",
    "#             response_last_ck = last\n",
    "        else:\n",
    "            response_hidden = torch.cat((response_hidden, last_hidden), dim=1)  # bze, n, hid_size\n",
    "#             response_last_ck = torch.cat((response_last_ck, last), dim=1)  # bze, n\n",
    "        if to_break:\n",
    "            break\n",
    "        \n",
    "        # manually assign end token is too long\n",
    "        if r_i == length - 1:\n",
    "            for r_m_b_i in range(batch_size):\n",
    "                if generated_response_length[r_m_b_i] == 0:\n",
    "                    last[r_m_b_i] = tokenizer.encode(\".\")[0]  # encode outputs a list (1 element)\n",
    "                    gumbel_vector[r_m_b_i] = F.one_hot(torch.tensor(tokenizer.encode(\".\"), dtype=torch.long, device=device), num_classes=vocab_size)\n",
    "        \n",
    "        # adjust \n",
    "        r_is_generated = torch.tensor(generated_prompt_length + 1 + batch_min_length) <= (r_i + cp_min_length)  # bze x 1. is generated or stil in the context+prompt\n",
    "        r_is_end_token = last == torch.tensor(tokenizer.encode(\".\"), device=device)  # bze x 1\n",
    "        r_is_actually_ending = r_is_generated * r_is_end_token\n",
    "#         print(\"r_is_generated; r_is_end_token; r_is_actually_ending\")\n",
    "#         print(r_is_generated)\n",
    "#         print(r_is_end_token)\n",
    "#         print(r_is_actually_ending)\n",
    "#         print()\n",
    "        # keep track of response length\n",
    "        generated_response_length = generated_response_length + response_not_done * r_is_actually_ending * r_i\n",
    "#         print(\"generated_response_length\")\n",
    "#         print(generated_response_length)\n",
    "#         print()\n",
    "        \n",
    "        # if generated, use the generated token as last; otherwise (from context+prompt), copy the orignal token/gumbel_vector\n",
    "#         print(\"last\\n: before\")\n",
    "#         print(last)\n",
    "#         for li in last.squeeze(-1).tolist():\n",
    "#             print(tokenizer.decode(li))\n",
    "        if cp_min_length + r_i < output_so_far.shape[1]:\n",
    "#             last = last * r_is_generated + output_so_far[:, cp_min_length + r_i].unsqueeze(1) * (1 - is_generated)\n",
    "            last = last * r_is_generated + output_so_far[:, cp_min_length + r_i].unsqueeze(1) * (~r_is_generated)\n",
    "        else:\n",
    "            last = last\n",
    "#         print(\"after\")\n",
    "#         print(last)\n",
    "#         for li in last.squeeze(-1).tolist():\n",
    "#             print(tokenizer.decode(li))\n",
    "#         print()\n",
    "        \n",
    "        if gumbel_softmax:\n",
    "            if cp_min_length - batch_min_length + r_i < all_gumbel_vectors.shape[1]:\n",
    "                # gumbel_vector = gumbel_vector * r_is_generated + all_gumbel_vectors[:, cp_min_length - batch_min_length + r_i, :] * (1 - is_generated)\n",
    "                gumbel_vector = gumbel_vector * r_is_generated + all_gumbel_vectors[:, cp_min_length - batch_min_length + r_i, :] * (~r_is_generated)                \n",
    "            else:\n",
    "                gumbel_vector = gumbel_vector\n",
    "        \n",
    "#         print(\"checking gumbel vector\")\n",
    "#         for li in range(last.shape[0]):\n",
    "#             print(gumbel_vector[li, last[li]])\n",
    "#             print(torch.max(gumbel_vector[li], dim=-1)[1])\n",
    "#         print()\n",
    "\n",
    "        everything_so_far = torch.cat((everything_so_far, last), dim=1)\n",
    "        \n",
    "#         for bi in range(batch_size):\n",
    "#             print(\"+++++%s+++++\" % bi)\n",
    "#             print(everything_so_far[bi])\n",
    "#             print(tokenizer.decode(everything_so_far[bi].tolist()))\n",
    "#         print()\n",
    "\n",
    "\n",
    "        # generate one sentence which ends with \".\" as the prompt\n",
    "#         if last.squeeze(0).data.cpu().numpy()[0] == tokenizer.encode(\".\")[0]:\n",
    "#             to_break = True\n",
    "            # break\n",
    "        # response_not_done = response_not_done * (1 - r_is_actually_ending)  # to check is we need to stop by summing\n",
    "        response_not_done = response_not_done * (~r_is_actually_ending)  # to check is we need to stop by summing\n",
    "        if torch.sum(response_not_done) == 0:\n",
    "            to_break = True\n",
    "            \n",
    "    print(\"response: \")\n",
    "#     print(generated_response_length.squeeze(-1).tolist())\n",
    "#     print(response_so_far.tolist())\n",
    "#     print(tokenizer.decode(response_so_far.tolist()[0]))\n",
    "#     print(\"***\" * 20)\n",
    "#     print(everything_so_far)\n",
    "    for pr_i in range(batch_size):\n",
    "#         print(pr_i)\n",
    "#         print(generated_response_length[pr_i])\n",
    "#         print(generated_response_length[pr_i] == 0)\n",
    "        if generated_response_length[pr_i] == 0:\n",
    "            print(tokenizer.decode(everything_so_far[pr_i][generated_prompt_length[pr_i].item() + 1 + batch_min_length:].tolist()))  # exceeds the max length set (so generated_prompt_length is 0)\n",
    "        else:\n",
    "            print(tokenizer.decode(everything_so_far[pr_i][generated_prompt_length[pr_i].item() + 1 + batch_min_length:(generated_response_length[pr_i].item() + 1 + cp_min_length)].tolist()))\n",
    "    print(\"***\" * 20)\n",
    "    \n",
    "    extracted_hidden = None\n",
    "    # hidden: bze, 1, hid_size\n",
    "    for hb_i in range(batch_size):\n",
    "        hb_i_start = generated_prompt_length[hb_i] + 1 + batch_min_length - cp_min_length + 1\n",
    "        hb_i_end = generated_response_length[hb_i] + 1 + 1\n",
    "#         print(hb_i, hb_i_start, hb_i_end)\n",
    "#         print(tokenizer.decode(response_last_ck[hb_i, hb_i_start:hb_i_end].tolist()))\n",
    "        # hb_i_hidden = torch.mean(torch.cat(response_hidden[hb_i:hb_i+1, hb_i_start:hb_i_end, :], dim=1)[hb_i:hb_i+1], dim=1) # 1, hid_size\n",
    "        hb_i_hidden = torch.mean(response_hidden[hb_i:hb_i+1, hb_i_start:hb_i_end, :], dim=1)  # 1, hid_size \n",
    "        if extracted_hidden is None:\n",
    "            extracted_hidden = hb_i_hidden\n",
    "        else:\n",
    "            extracted_hidden = torch.cat((extracted_hidden, hb_i_hidden), dim=0)\n",
    "#     print()\n",
    "#     assert False\n",
    "    \n",
    "    prediction = classifier(extracted_hidden)\n",
    "    label = torch.tensor([class_label], device=device, dtype=torch.long).repeat(batch_size)\n",
    "    discrim_loss = ce_loss(prediction, label)\n",
    "\n",
    "    # debuggin\n",
    "    # print(\"hidden\")\n",
    "    # print(torch.cat(response_hidden, dim=1).shape)\n",
    "    # print(torch.cat(response_hidden, dim=1))\n",
    "    # print()\n",
    "    # print(\"attn\")\n",
    "    # print(lm_rep_output[\"attentions\"][-1].shape)\n",
    "    # # print(lm_rep_output[\"attentions\"][-1][0][0])\n",
    "    # print(lm_rep_output[\"attentions\"][0][0][0])\n",
    "    # print(\"past\")\n",
    "    # print(\"key\")  # bze, n_heads, seq, head_dim\n",
    "    # print(past[-1][0][0][0, :, :5])\n",
    "    # print(\"layer 0\")\n",
    "    # print(past[0][0][0][0, :, :5])\n",
    "\n",
    "\n",
    "\n",
    "    # debugging\n",
    "    # print(\"prediction: \")\n",
    "    # print(prediction)\n",
    "    print(\"discrim loss: %.6f\" % discrim_loss.data.cpu().numpy())\n",
    "#     print(prediction)\n",
    "#     print(label)\n",
    "#     print(discrim_loss)\n",
    "    loss_per_update += discrim_loss.item()\n",
    "\n",
    "    if num_of_triggers > 0:\n",
    "        # compute gradients\n",
    "        discrim_loss.backward()\n",
    "\n",
    "        # # debugging: check grad\n",
    "        if trigger_format == \"token\":\n",
    "            print_debug = False\n",
    "            if print_debug:\n",
    "                print(\"token grad\")\n",
    "                print(model.ori_trigger_embedding.grad)\n",
    "    #             print(trigger_embedding)\n",
    "            #\n",
    "                # debugging\n",
    "                print(\"original trigger embedding\")\n",
    "                print(model.ori_trigger_embedding)\n",
    "        # else:\n",
    "        #     print(\"trigger_key_value_grad\")\n",
    "        #     # print(model.l_12_key_0.grad.shape)\n",
    "        #     # print(model.l_12_key_0.grad)\n",
    "        #     print(model.l_12_value.grad.shape)\n",
    "        #     # print(model.l_12_value_1.grad)\n",
    "        #     print(model.l_12_value)\n",
    "\n",
    "        if (i + 1) % gradient_accumulation_steps == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            \n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n",
    "            print(\"\\n=======update loss: %.6f=======\" % (loss_per_update / gradient_accumulation_steps))\n",
    "            total_loss += loss_per_update\n",
    "            loss_per_update = 0\n",
    "\n",
    "        # # debugging\n",
    "        # print(\"new trigger embedding\")\n",
    "        # print(trigger_embedding)\n",
    "        \n",
    "#         print(model.ori_trigger_key_values)\n",
    "\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "print(\"\\n\\ntotal average loss: %.6f\" % (total_loss / num_iterations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dianyu/anaconda3/envs/zero/lib/python3.6/site-packages/ipykernel_launcher.py:63: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real responses: \n",
      "<|endoftext|>My favorite music genre is death metal. In my mind, death metal is all about the heavy death metal. If you can't see it, you are missing it.\n",
      "<|endoftext|>I listen to rap music. I love it. It reminds me of the good times.\n",
      "<|endoftext|>I workout four hours a day. When I get into the gym, I'm not going to lie, it's not a lot. My heart rate is high, the adrenaline rush is high.\n",
      "<|endoftext|>I'm a christian. So I'm not going to pretend to know what the Bible is about, but I do know that if it's about Jesus as the Son of God, then I have a lot of respect for. I'm not.\n",
      "************************************************************\n",
      "loss\n",
      "tensor(2.9984, device='cuda:0')\n",
      "\n",
      "Real responses: \n",
      "<|endoftext|>My favorite music genre is death metal. The genre was born out of the need for something that would feel right in your stomach and not feel too weird, yet not feel too weird for you to listen to while eating. In my opinion, metal can have the best music.\n",
      "<|endoftext|>I listen to rap music. I don't listen to music as an artform, but to music as a medium. It is my music.\n",
      "<|endoftext|>I workout four hours a day. I am an avid weightlifter. I have trained for almost 40 years.\n",
      "<|endoftext|>I'm a christian. I believe in Jesus and love Jesus and want him to be my king, my prophet, our savior. I am an atheist so when I was a kid my mom and dad were not only atheists but were also not allowed to have any religious.\n",
      "************************************************************\n",
      "loss\n",
      "tensor(2.2931, device='cuda:0')\n",
      "\n",
      "all_loss\n",
      "2.645717144012451\n"
     ]
    }
   ],
   "source": [
    "# move to the beginning\n",
    "# sample_batch_size = 8\n",
    "\n",
    "def sample_gpt2(model, tokenizer, all_context_prompts, all_generated_prompt_length, \n",
    "                padding_token, classifier, class_label):\n",
    "    real_loss_all = 0\n",
    "    for real_i in range(len(all_context_prompts)):\n",
    "        cp_length_i = all_generated_prompt_length[real_i]\n",
    "        min_cp_length_i = torch.min(cp_length_i)  # tensor of float (no shape)\n",
    "        max_cp_length_i = torch.max(cp_length_i)\n",
    "        cp_i_text = all_context_prompts[real_i]\n",
    "        \n",
    "        cp_i_input_ids = list()\n",
    "        for cp_i_j in cp_i_text:\n",
    "            input_ids = tokenizer.encode(cp_i_j)\n",
    "            padding_len = max_cp_length_i - len(input_ids)\n",
    "            input_ids = input_ids + [padding_token] * padding_len\n",
    "            cp_i_input_ids.append(input_ids)\n",
    "        cp_i_inputs = torch.tensor(cp_i_input_ids, dtype=torch.long, device=device)\n",
    "        \n",
    "        cp_i_batch_size = len(cp_i_text)\n",
    "        real_response_hidden = None\n",
    "        real_rp_h_ck = None\n",
    "        to_break = False\n",
    "        cp_i_response_not_done = torch.ones(cp_i_batch_size, 1, dtype=torch.uint8, device=device)\n",
    "        cp_i_generated_response_length = torch.zeros(cp_i_batch_size, 1, dtype=torch.long, device=device)\n",
    "        \n",
    "        with torch.no_grad():  # Need this?\n",
    "            cp_i_so_far = cp_i_inputs[:, :min_cp_length_i]  # bze x (min_cp_length_i - 1)\n",
    "            cp_i_context_lm_output = model(cp_i_inputs[:, :min_cp_length_i - 1])\n",
    "            past = cp_i_context_lm_output[\"past_key_values\"]\n",
    "            last = cp_i_inputs[:, min_cp_length_i - 1: min_cp_length_i]\n",
    "            \n",
    "            for rr_i in range(length):  # length + max_cp_length_i - min_cp_length_i?\n",
    "                lm_cp_i_output = model(last, past_key_values=past)\n",
    "                cp_i_logits, past, cp_i_hidden = (lm_cp_i_output[\"logits\"],  # bze, cur_seq_len, vocab_size\n",
    "                                                 lm_cp_i_output[\"past_key_values\"],  # acc_seq_len\n",
    "                                                 lm_cp_i_output[\"hidden_states\"])  # num_layers + 1, tuple of (bze, cur_seq_len, hid_sze)\n",
    "                                \n",
    "                cp_i_logits = penalize_new_line(cp_i_logits)\n",
    "                \n",
    "                last, _ = generate_next(cp_i_logits, cp_i_so_far, top_k=top_k, temperature=temperature, \n",
    "                                        repetition_penalty=repetition_penalty, sample=sample,\n",
    "                                        gumbel_softmax=False, gumbel_temperature=gumbel_temperature, detach=False)\n",
    "                \n",
    "                cp_i_last_hidden = cp_i_hidden[-1]\n",
    "                if real_response_hidden is None:\n",
    "                    real_response_hidden = cp_i_last_hidden\n",
    "                    real_rp_h_ck = last\n",
    "                else:\n",
    "                    real_response_hidden = torch.cat((real_response_hidden, cp_i_last_hidden), dim=1)  # bze, n, hid_size\n",
    "                    real_rp_h_ck = torch.cat((real_rp_h_ck, last), dim=1)  # bze, n  \n",
    "                if to_break:\n",
    "                    break\n",
    "                    \n",
    "                # manually assign end token if too long\n",
    "                if rr_i == length - 1:\n",
    "                    for rr_b_i in range(cp_i_batch_size):\n",
    "                        if cp_i_generated_response_length[rr_b_i] == 0:\n",
    "                            last[rr_b_i] = tokenizer.encode(\".\")[0]  # encode outputs a list (1 element)\n",
    "            \n",
    "                # adjust\n",
    "                rr_i_is_generated = torch.tensor(cp_length_i) <= (rr_i + min_cp_length_i)  # bze x 1. is generated or stil in the context+prompt\n",
    "                rr_i_is_end_token = last == torch.tensor(tokenizer.encode(\".\"), device=device)  # bze x 1\n",
    "                rr_i_is_actually_ending = rr_i_is_generated * rr_i_is_end_token\n",
    "                \n",
    "                # keep track of response length\n",
    "                cp_i_generated_response_length = cp_i_generated_response_length + cp_i_response_not_done * rr_i_is_actually_ending * rr_i\n",
    "                \n",
    "                # if generated, use the generated token as last; otherwise (from context+prompt), copy the orignal token/gumbel_vector\n",
    "                if min_cp_length_i + rr_i < max_cp_length_i:\n",
    "                    last = last * rr_i_is_generated + cp_i_inputs[:, min_cp_length_i + rr_i].unsqueeze(1) * (~rr_i_is_generated)\n",
    "                else:\n",
    "                    last = last\n",
    "                \n",
    "                cp_i_so_far = torch.cat((cp_i_so_far, last), dim=1)\n",
    "                \n",
    "                cp_i_response_not_done = cp_i_response_not_done * (~rr_i_is_actually_ending)\n",
    "                if torch.sum(cp_i_response_not_done) == 0:\n",
    "                    to_break = True\n",
    "            \n",
    "            print(\"Real responses: \")\n",
    "            for r_pr_i in range(cp_i_batch_size):\n",
    "                if cp_i_generated_response_length[r_pr_i] == 0:\n",
    "                    print(tokenizer.decode(cp_i_so_far[r_pr_i, :].tolist()))  # exceeds the max length set (so generated_prompt_length is 0)\n",
    "                else:\n",
    "                    print(tokenizer.decode(cp_i_so_far[r_pr_i, :(cp_i_generated_response_length[r_pr_i].item() + 1 + min_cp_length_i)].tolist()))\n",
    "            print(\"***\" * 20)\n",
    "            \n",
    "            cp_i_extracted_hidden = None\n",
    "            for r_hb_i in range(cp_i_batch_size):\n",
    "                r_hb_i_start = cp_length_i[r_hb_i] - min_cp_length_i + 1\n",
    "                r_hb_i_end = cp_i_generated_response_length[r_hb_i] + 1 + 1\n",
    "#                 print(r_hb_i, r_hb_i_start, r_hb_i_end)\n",
    "#                 print(tokenizer.decode(real_rp_h_ck[r_hb_i, r_hb_i_start:r_hb_i_end].tolist()))\n",
    "                r_hb_i_hidden = torch.mean(real_response_hidden[r_hb_i:r_hb_i+1, r_hb_i_start:r_hb_i_end, :], dim=1)\n",
    "                if cp_i_extracted_hidden is None:\n",
    "                    cp_i_extracted_hidden = r_hb_i_hidden\n",
    "                else:\n",
    "                    cp_i_extracted_hidden = torch.cat((cp_i_extracted_hidden, r_hb_i_hidden), dim=0)\n",
    "            \n",
    "            prediction = classifier(cp_i_extracted_hidden)\n",
    "            label = torch.tensor([class_label], device=device, dtype=torch.long).repeat(cp_i_batch_size)\n",
    "            loss = ce_loss(prediction, label)\n",
    "            \n",
    "            real_loss_all += loss.item()\n",
    "                \n",
    "            print(\"loss\")\n",
    "            print(loss)\n",
    "            print()\n",
    "            \n",
    "            # WARNING: Debugging\n",
    "#             break\n",
    "    print(\"all_loss\")\n",
    "    print(real_loss_all / len(all_context_prompts))\n",
    "\n",
    "sample_gpt2(model, tokenizer, all_context_prompts, all_generated_prompt_length, padding_token, classifier, class_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_context_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zero",
   "language": "python",
   "name": "zero"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
